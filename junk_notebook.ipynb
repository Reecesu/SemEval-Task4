{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Libraries..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import bs4\n",
    "import bert\n",
    "import nltk\n",
    "import spacy\n",
    "import string\n",
    "import keras\n",
    "from empath import Empath\n",
    "import en_core_web_trf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import *\n",
    "from keras.layers import *\n",
    "import collections\n",
    "import tf_slim as slim\n",
    "import transformers\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from keras.initializers import Constant\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from bert import optimization\n",
    "from bert import tokenization\n",
    "\n",
    "\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "arguments_training = pd.read_csv('arguments-training.tsv', sep='\\t')\n",
    "labels_training = pd.read_csv('labels-training.tsv', sep='\\t')\n",
    "level1_labels_training = pd.read_csv('level1-labels-training.tsv', sep='\\t')\n",
    "\n",
    "# load value-categories.json to json object\n",
    "import json\n",
    "with open('value-categories.json') as f:\n",
    "    value_categories = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Argument ID</th>\n",
       "      <th>Conclusion</th>\n",
       "      <th>Stance</th>\n",
       "      <th>Premise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A01001</td>\n",
       "      <td>Entrapment should be legalized</td>\n",
       "      <td>in favor of</td>\n",
       "      <td>if entrapment can serve to more easily capture...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A01002</td>\n",
       "      <td>We should ban human cloning</td>\n",
       "      <td>in favor of</td>\n",
       "      <td>we should ban human cloning as it will only ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A01003</td>\n",
       "      <td>We should abandon marriage</td>\n",
       "      <td>against</td>\n",
       "      <td>marriage is the ultimate commitment to someone...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A01004</td>\n",
       "      <td>We should ban naturopathy</td>\n",
       "      <td>against</td>\n",
       "      <td>it provides a useful income for some people</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A01005</td>\n",
       "      <td>We should ban fast food</td>\n",
       "      <td>in favor of</td>\n",
       "      <td>fast food should be banned because it is reall...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Argument ID                      Conclusion       Stance  \\\n",
       "0      A01001  Entrapment should be legalized  in favor of   \n",
       "1      A01002     We should ban human cloning  in favor of   \n",
       "2      A01003      We should abandon marriage      against   \n",
       "3      A01004       We should ban naturopathy      against   \n",
       "4      A01005         We should ban fast food  in favor of   \n",
       "\n",
       "                                             Premise  \n",
       "0  if entrapment can serve to more easily capture...  \n",
       "1  we should ban human cloning as it will only ca...  \n",
       "2  marriage is the ultimate commitment to someone...  \n",
       "3        it provides a useful income for some people  \n",
       "4  fast food should be banned because it is reall...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arguments_training.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Argument ID</th>\n",
       "      <th>Self-direction: thought</th>\n",
       "      <th>Self-direction: action</th>\n",
       "      <th>Stimulation</th>\n",
       "      <th>Hedonism</th>\n",
       "      <th>Achievement</th>\n",
       "      <th>Power: dominance</th>\n",
       "      <th>Power: resources</th>\n",
       "      <th>Face</th>\n",
       "      <th>Security: personal</th>\n",
       "      <th>...</th>\n",
       "      <th>Tradition</th>\n",
       "      <th>Conformity: rules</th>\n",
       "      <th>Conformity: interpersonal</th>\n",
       "      <th>Humility</th>\n",
       "      <th>Benevolence: caring</th>\n",
       "      <th>Benevolence: dependability</th>\n",
       "      <th>Universalism: concern</th>\n",
       "      <th>Universalism: nature</th>\n",
       "      <th>Universalism: tolerance</th>\n",
       "      <th>Universalism: objectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A01001</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A01002</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A01003</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A01004</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A01005</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Argument ID  Self-direction: thought  Self-direction: action  Stimulation  \\\n",
       "0      A01001                        0                       0            0   \n",
       "1      A01002                        0                       0            0   \n",
       "2      A01003                        0                       1            0   \n",
       "3      A01004                        0                       0            0   \n",
       "4      A01005                        0                       0            0   \n",
       "\n",
       "   Hedonism  Achievement  Power: dominance  Power: resources  Face  \\\n",
       "0         0            0                 0                 0     0   \n",
       "1         0            0                 0                 0     0   \n",
       "2         0            0                 0                 0     0   \n",
       "3         0            0                 0                 0     0   \n",
       "4         0            0                 0                 0     0   \n",
       "\n",
       "   Security: personal  ...  Tradition  Conformity: rules  \\\n",
       "0                   0  ...          0                  0   \n",
       "1                   0  ...          0                  0   \n",
       "2                   0  ...          0                  0   \n",
       "3                   1  ...          0                  0   \n",
       "4                   1  ...          0                  0   \n",
       "\n",
       "   Conformity: interpersonal  Humility  Benevolence: caring  \\\n",
       "0                          0         0                    0   \n",
       "1                          0         0                    0   \n",
       "2                          0         0                    0   \n",
       "3                          0         0                    0   \n",
       "4                          0         0                    0   \n",
       "\n",
       "   Benevolence: dependability  Universalism: concern  Universalism: nature  \\\n",
       "0                           0                      0                     0   \n",
       "1                           0                      0                     0   \n",
       "2                           0                      0                     0   \n",
       "3                           0                      0                     0   \n",
       "4                           0                      0                     0   \n",
       "\n",
       "   Universalism: tolerance  Universalism: objectivity  \n",
       "0                        0                          0  \n",
       "1                        0                          0  \n",
       "2                        0                          0  \n",
       "3                        0                          0  \n",
       "4                        0                          0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_training.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"Self-direction: thought\": {\n",
      "        \"Be creative\": [\n",
      "            \"allowing for more creativity or imagination\",\n",
      "            \"being more creative\",\n",
      "            \"fostering creativity\",\n",
      "            \"promoting imagination\"\n",
      "        ],\n",
      "        \"Be curious\": [\n",
      "            \"being the more interesting option\",\n",
      "            \"fostering curiosity\",\n",
      "            \"making people more keen to learn\",\n",
      "       \n"
     ]
    }
   ],
   "source": [
    "# print structure of value-categories.json\n",
    "print(json.dumps(value_categories, indent=4)[:400])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roomba(item):\n",
    "    '''\n",
    "    input: string\n",
    "    output: list of cleaned words\n",
    "\n",
    "    '''\n",
    "    soup = bs4.BeautifulSoup(item, 'html.parser')\n",
    "    text = soup.get_text()\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "    text = re.sub(r'\\[.*\\]\\(.*\\)', '', text)\n",
    "\n",
    "    # remobe '[removed]' and '[deleted]'\n",
    "    text = re.sub(r'\\[.*\\]', '', text)\n",
    "\n",
    "    # remove non utf-8 characters\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "\n",
    "    # lowercase \n",
    "    text = text.lower()\n",
    "\n",
    "    # remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    # remove leading and trailing spaces\n",
    "    text = text.strip()\n",
    "\n",
    "    # remove urls\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "\n",
    "    # remove stopwords\n",
    "    text = [word for word in text.split() if word not in stopwords]\n",
    "    text = [lemmatizer.lemmatize(word) for word in text]\n",
    "\n",
    "\n",
    "    if text == []:\n",
    "        return False\n",
    "    else:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# create a tokenizer with NLTK\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokenizer = word_tokenize\n",
    "\n",
    "# create a lemmatizer with NLTK\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sucho\\anaconda3\\envs\\py39\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# run the roomba function on the arguments_training dataframe append a 'cleaned' column\n",
    "arguments_training['Cleaned_Premise'] = arguments_training['Premise'].apply(roomba)\n",
    "arguments_training['Cleaned_Conclusion'] = arguments_training['Conclusion'].apply(roomba)\n",
    "\n",
    "# combine 'conclusion', 'premise', and 'stance' into one column, then run roomba function\n",
    "arguments_training['Cleaned_BOW'] = arguments_training['Premise'] + ' ' + arguments_training['Conclusion'] + ' ' + arguments_training['Stance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Argument ID</th>\n",
       "      <th>Conclusion</th>\n",
       "      <th>Stance</th>\n",
       "      <th>Premise</th>\n",
       "      <th>Cleaned_Premise</th>\n",
       "      <th>Cleaned_Conclusion</th>\n",
       "      <th>Cleaned_BOW</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A01001</td>\n",
       "      <td>Entrapment should be legalized</td>\n",
       "      <td>in favor of</td>\n",
       "      <td>if entrapment can serve to more easily capture...</td>\n",
       "      <td>[entrapment, serve, easily, capture, wanted, c...</td>\n",
       "      <td>[entrapment, legalized]</td>\n",
       "      <td>if entrapment can serve to more easily capture...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A01002</td>\n",
       "      <td>We should ban human cloning</td>\n",
       "      <td>in favor of</td>\n",
       "      <td>we should ban human cloning as it will only ca...</td>\n",
       "      <td>[ban, human, cloning, cause, huge, issue, bunc...</td>\n",
       "      <td>[ban, human, cloning]</td>\n",
       "      <td>we should ban human cloning as it will only ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A01003</td>\n",
       "      <td>We should abandon marriage</td>\n",
       "      <td>against</td>\n",
       "      <td>marriage is the ultimate commitment to someone...</td>\n",
       "      <td>[marriage, ultimate, commitment, someone, peop...</td>\n",
       "      <td>[abandon, marriage]</td>\n",
       "      <td>marriage is the ultimate commitment to someone...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A01004</td>\n",
       "      <td>We should ban naturopathy</td>\n",
       "      <td>against</td>\n",
       "      <td>it provides a useful income for some people</td>\n",
       "      <td>[provides, useful, income, people]</td>\n",
       "      <td>[ban, naturopathy]</td>\n",
       "      <td>it provides a useful income for some people We...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A01005</td>\n",
       "      <td>We should ban fast food</td>\n",
       "      <td>in favor of</td>\n",
       "      <td>fast food should be banned because it is reall...</td>\n",
       "      <td>[fast, food, banned, really, bad, health, costly]</td>\n",
       "      <td>[ban, fast, food]</td>\n",
       "      <td>fast food should be banned because it is reall...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Argument ID                      Conclusion       Stance  \\\n",
       "0      A01001  Entrapment should be legalized  in favor of   \n",
       "1      A01002     We should ban human cloning  in favor of   \n",
       "2      A01003      We should abandon marriage      against   \n",
       "3      A01004       We should ban naturopathy      against   \n",
       "4      A01005         We should ban fast food  in favor of   \n",
       "\n",
       "                                             Premise  \\\n",
       "0  if entrapment can serve to more easily capture...   \n",
       "1  we should ban human cloning as it will only ca...   \n",
       "2  marriage is the ultimate commitment to someone...   \n",
       "3        it provides a useful income for some people   \n",
       "4  fast food should be banned because it is reall...   \n",
       "\n",
       "                                     Cleaned_Premise       Cleaned_Conclusion  \\\n",
       "0  [entrapment, serve, easily, capture, wanted, c...  [entrapment, legalized]   \n",
       "1  [ban, human, cloning, cause, huge, issue, bunc...    [ban, human, cloning]   \n",
       "2  [marriage, ultimate, commitment, someone, peop...      [abandon, marriage]   \n",
       "3                 [provides, useful, income, people]       [ban, naturopathy]   \n",
       "4  [fast, food, banned, really, bad, health, costly]        [ban, fast, food]   \n",
       "\n",
       "                                         Cleaned_BOW  \n",
       "0  if entrapment can serve to more easily capture...  \n",
       "1  we should ban human cloning as it will only ca...  \n",
       "2  marriage is the ultimate commitment to someone...  \n",
       "3  it provides a useful income for some people We...  \n",
       "4  fast food should be banned because it is reall...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arguments_training.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge labels-training and arguments-training on ID\n",
    "training_data = pd.merge(labels_training, arguments_training, on='Argument ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Argument ID', 'Self-direction: thought', 'Self-direction: action',\n",
       "       'Stimulation', 'Hedonism', 'Achievement', 'Power: dominance',\n",
       "       'Power: resources', 'Face', 'Security: personal', 'Security: societal',\n",
       "       'Tradition', 'Conformity: rules', 'Conformity: interpersonal',\n",
       "       'Humility', 'Benevolence: caring', 'Benevolence: dependability',\n",
       "       'Universalism: concern', 'Universalism: nature',\n",
       "       'Universalism: tolerance', 'Universalism: objectivity', 'Conclusion',\n",
       "       'Stance', 'Premise', 'Cleaned_Premise', 'Cleaned_Conclusion',\n",
       "       'Cleaned_BOW'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import embedding models\n",
    "path_to_gloVe_file = \"glove.6B.200d.txt\"\n",
    "\n",
    "# dictionary of words to embeddings\n",
    "embeddings_index = {}\n",
    "\n",
    "with open(path_to_gloVe_file, encoding='utf8') as f:\n",
    "  for line in f:\n",
    "    word, coefs = line.split(maxsplit = 1)\n",
    "    coefs = np.fromstring(coefs, \"f\", sep = \" \")\n",
    "    embeddings_index[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find argument with longest length of words\n",
    "max_tokens = training_data['Cleaned_BOW'].str.split().str.len().max()\n",
    "data = training_data['Cleaned_BOW']\n",
    "\n",
    "# This text vectorizer indexs our vocabulary based on the train samples\n",
    "vectorizer = TextVectorization(max_tokens = max_tokens, output_sequence_length = 100, split = 'whitespace')\n",
    "data_tensor = tf.data.Dataset.from_tensor_slices(data).batch(32)\n",
    "vectorizer.adapt(data_tensor)\n",
    "\n",
    "# map unique words to integers\n",
    "vocabulary = vectorizer.get_vocabulary()\n",
    "index = dict(zip(vocabulary, range(len(vocabulary))))\n",
    "len_vocabulary = len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding coverage:  97.92 %\n",
      "Captured words:  141\n",
      "Missed words:  3\n"
     ]
    }
   ],
   "source": [
    "# create embedding matrix\n",
    "embedding_dim = 200\n",
    "\n",
    "# create a counter of words in the training set also in the embedding set\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# prepare embedding matrix for embedding layer (ref. code tutorial 7)\n",
    "embedding_matrix = np.zeros((len_vocabulary, embedding_dim))\n",
    "for word, i in index.items():\n",
    "  embedding_vector = embeddings_index.get(word)\n",
    "  if embedding_vector is not None:\n",
    "    embedding_matrix[i] = embedding_vector\n",
    "    hits+=1\n",
    "  else:\n",
    "    misses+=1\n",
    "\n",
    "# create embedding layer for our network\n",
    "embedding_layer = Embedding(len_vocabulary, embedding_dim, embeddings_initializer=Constant(embedding_matrix), trainable = False)\n",
    "\n",
    "# print ratio of words in embedding set to words in training set\n",
    "print(\"Embedding coverage: \", round((hits/(hits+misses))*100,2),\"%\")\n",
    "print(\"Captured words: \", hits)\n",
    "print(\"Missed words: \", misses)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_category = list(value_categories.keys())\n",
    "# category_value = list(value_categories.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Self-direction: thought',\n",
       " 'Self-direction: action',\n",
       " 'Stimulation',\n",
       " 'Hedonism',\n",
       " 'Achievement',\n",
       " 'Power: dominance',\n",
       " 'Power: resources',\n",
       " 'Face',\n",
       " 'Security: personal',\n",
       " 'Security: societal',\n",
       " 'Tradition',\n",
       " 'Conformity: rules',\n",
       " 'Conformity: interpersonal',\n",
       " 'Humility',\n",
       " 'Benevolence: caring',\n",
       " 'Benevolence: dependability',\n",
       " 'Universalism: concern',\n",
       " 'Universalism: nature',\n",
       " 'Universalism: tolerance',\n",
       " 'Universalism: objectivity']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['allowing', 'for', 'more', 'creativity', 'or', 'imagination', 'being', 'more', 'creative', 'fostering']\n",
      "['allowing', 'creativity', 'imagination', 'creative', 'fostering', 'creativity', 'promoting', 'imagination', 'interesting', 'option']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# create an empty dictionary to store the tokens\n",
    "tokens = {}\n",
    "\n",
    "# iterate over the human value categories in the data\n",
    "for category, values in value_categories.items():\n",
    "  # create an empty list to store the tokens for the category\n",
    "  tokens[category] = []\n",
    "  # iterate over the examples of expressions for the category\n",
    "  for examples in values.values():\n",
    "    # flatten the list of examples into a single-level list of tokens\n",
    "    tokens[category].extend([token for example in examples for token in example.split()])\n",
    "\n",
    "# print the first 10 tokens for the category 'Self-direction: thought'\n",
    "print(tokens['Self-direction: thought'][:10])\n",
    "\n",
    "# pre-process tokens\n",
    "for category in tokens.keys():\n",
    "  # convert to lowercase\n",
    "  tokens[category] = [token.lower() for token in tokens[category]]\n",
    "  # remove stop words\n",
    "  tokens[category] = [token for token in tokens[category] if token not in stop_words]\n",
    "  # lemmatize\n",
    "  tokens[category] = [lemmatizer.lemmatize(token) for token in tokens[category]]\n",
    "\n",
    "# print the first 10 processed tokens for the category 'Self-direction: thought'\n",
    "print(tokens['Self-direction: thought'][:10])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of all value categories and subcategories\n",
    "all_categories = []\n",
    "for category, subcategories in value_categories.items():\n",
    "    all_categories.append(category)\n",
    "    for subcategory, _ in subcategories.items():\n",
    "        all_categories.append(subcategory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the arguments and labels data from the tsv files\n",
    "arguments_data = {}\n",
    "with open('arguments-training.tsv') as f:\n",
    "    for line in f:\n",
    "        argument_id, conclusion, stance, premise = line.strip().split('\\t')\n",
    "        arguments_data[argument_id] = {\n",
    "            'premise': premise,\n",
    "            'conclusion': conclusion,\n",
    "            'stance': stance,\n",
    "            'categories': []\n",
    "        }\n",
    "\n",
    "with open('labels-training.tsv') as f:\n",
    "    for line in f:\n",
    "        argument_id, *labels = line.strip().split('\\t')\n",
    "        arguments_data[argument_id]['categories'] = [\n",
    "            category\n",
    "            for category, label in zip(all_categories, labels)\n",
    "            if label == '1'\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of all arguments and their corresponding categories and subcategories\n",
    "arguments = []\n",
    "categories = []\n",
    "for argument_id, argument_data in arguments_data.items():\n",
    "    premise = argument_data['premise']\n",
    "    conclusion = argument_data['conclusion']\n",
    "    stance = argument_data['stance']\n",
    "\n",
    "    # create the argument text by concatenating the premise and conclusion\n",
    "    argument_text = premise + ' ' + conclusion\n",
    "\n",
    "    # create a binary feature for each value category and subcategory\n",
    "    argument_categories = []\n",
    "    for category in all_categories:\n",
    "        if category in argument_data['categories']:\n",
    "            argument_categories.append(1)\n",
    "        else:\n",
    "            argument_categories.append(0)\n",
    "\n",
    "    # add the argument text and categories to the list\n",
    "    arguments.append(argument_text)\n",
    "    categories.append(argument_categories)\n",
    "\n",
    "# create a CountVectorizer to convert the argument text to numeric features\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(arguments)\n",
    "\n",
    "# combine the numeric features with the binary category features\n",
    "X_with_categories = np.hstack((X.todense(), np.array(categories)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['r', 'g', 'u', 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], ['r', 'g', 'u', 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], ['r', 'g', 'u', 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], ['r', 'g', 'u', 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], ['r', 'g', 'u', 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "value_category_list = list(value_categories.keys())\n",
    "\n",
    "# create empty list to store feature vectors\n",
    "feature_vectors = []\n",
    "\n",
    "# iterate over arguments\n",
    "for argument in arguments_training:\n",
    "    argument_id = argument[0]\n",
    "    conclusion = argument[1]\n",
    "    stance = argument[2]\n",
    "    premise = argument[3]\n",
    "\n",
    "    # create empty feature vector\n",
    "    feature_vector = []\n",
    "\n",
    "    # add conclusion, stance, and premise to feature vector\n",
    "    feature_vector.append(conclusion)\n",
    "    feature_vector.append(stance)\n",
    "    feature_vector.append(premise)\n",
    "\n",
    "    # add length of conclusion, stance, and premise to feature vector\n",
    "    feature_vector.append(len(conclusion))\n",
    "    feature_vector.append(len(stance))\n",
    "    feature_vector.append(len(premise))\n",
    "\n",
    "    # add number of words in conclusion, stance, and premise to feature vector\n",
    "    feature_vector.append(len(conclusion.split()))\n",
    "    feature_vector.append(len(stance.split()))\n",
    "    feature_vector.append(len(premise.split()))\n",
    "\n",
    "    # add value category information to feature vector\n",
    "    for value_category in value_category_list:\n",
    "        if argument_id in labels_training and value_category in labels_training[argument_id]:\n",
    "            feature_vector.append(1)\n",
    "        else:\n",
    "            feature_vector.append(0)\n",
    "\n",
    "        # append feature vector to list of feature vectors\n",
    "        feature_vectors.append(feature_vector)\n",
    "\n",
    "# print first 5 feature vectors\n",
    "print(feature_vectors[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sucho\\anaconda3\\envs\\py39\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:878: UserWarning: unknown class(es) ['Achievement', 'Benevolence: caring', 'Benevolence: dependability', 'Conformity: interpersonal', 'Conformity: rules', 'Face', 'Hedonism', 'Humility', 'Power: dominance', 'Power: resources', 'Security: personal', 'Security: societal', 'Self-direction: action', 'Self-direction: thought', 'Stimulation', 'Tradition', 'Universalism: concern', 'Universalism: nature', 'Universalism: objectivity', 'Universalism: tolerance'] will be ignored\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [5220, 20]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [37], line 53\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39m# Train a classifier to classify the arguments according to their values\u001b[39;00m\n\u001b[0;32m     52\u001b[0m classifier \u001b[39m=\u001b[39m OneVsRestClassifier(SVC(gamma\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m---> 53\u001b[0m classifier\u001b[39m.\u001b[39;49mfit(features, labels_bin)\n\u001b[0;32m     55\u001b[0m \u001b[39m# Evaluate the model on the test set\u001b[39;00m\n\u001b[0;32m     56\u001b[0m arguments_test \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39marguments-test.tsv\u001b[39m\u001b[39m'\u001b[39m, sep\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\sucho\\anaconda3\\envs\\py39\\lib\\site-packages\\sklearn\\multiclass.py:327\u001b[0m, in \u001b[0;36mOneVsRestClassifier.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    323\u001b[0m columns \u001b[39m=\u001b[39m (col\u001b[39m.\u001b[39mtoarray()\u001b[39m.\u001b[39mravel() \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m Y\u001b[39m.\u001b[39mT)\n\u001b[0;32m    324\u001b[0m \u001b[39m# In cases where individual estimators are very fast to train setting\u001b[39;00m\n\u001b[0;32m    325\u001b[0m \u001b[39m# n_jobs > 1 in can results in slower performance due to the overhead\u001b[39;00m\n\u001b[0;32m    326\u001b[0m \u001b[39m# of spawning threads.  See joblib issue #112.\u001b[39;00m\n\u001b[1;32m--> 327\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_ \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs, verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose)(\n\u001b[0;32m    328\u001b[0m     delayed(_fit_binary)(\n\u001b[0;32m    329\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mestimator,\n\u001b[0;32m    330\u001b[0m         X,\n\u001b[0;32m    331\u001b[0m         column,\n\u001b[0;32m    332\u001b[0m         classes\u001b[39m=\u001b[39;49m[\n\u001b[0;32m    333\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mnot \u001b[39;49m\u001b[39m%s\u001b[39;49;00m\u001b[39m\"\u001b[39;49m \u001b[39m%\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_binarizer_\u001b[39m.\u001b[39;49mclasses_[i],\n\u001b[0;32m    334\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_binarizer_\u001b[39m.\u001b[39;49mclasses_[i],\n\u001b[0;32m    335\u001b[0m         ],\n\u001b[0;32m    336\u001b[0m     )\n\u001b[0;32m    337\u001b[0m     \u001b[39mfor\u001b[39;49;00m i, column \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(columns)\n\u001b[0;32m    338\u001b[0m )\n\u001b[0;32m    340\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_[\u001b[39m0\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39mn_features_in_\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    341\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mn_features_in_\n",
      "File \u001b[1;32mc:\\Users\\sucho\\anaconda3\\envs\\py39\\lib\\site-packages\\joblib\\parallel.py:1085\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1076\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1077\u001b[0m     \u001b[39m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[0;32m   1078\u001b[0m     \u001b[39m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1082\u001b[0m     \u001b[39m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[0;32m   1083\u001b[0m     \u001b[39m# remaining jobs.\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m-> 1085\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1088\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[1;32mc:\\Users\\sucho\\anaconda3\\envs\\py39\\lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[0;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sucho\\anaconda3\\envs\\py39\\lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[0;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mc:\\Users\\sucho\\anaconda3\\envs\\py39\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m     \u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mc:\\Users\\sucho\\anaconda3\\envs\\py39\\lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[1;32mc:\\Users\\sucho\\anaconda3\\envs\\py39\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\sucho\\anaconda3\\envs\\py39\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\sucho\\anaconda3\\envs\\py39\\lib\\site-packages\\sklearn\\utils\\fixes.py:117\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    116\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig):\n\u001b[1;32m--> 117\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\sucho\\anaconda3\\envs\\py39\\lib\\site-packages\\sklearn\\multiclass.py:83\u001b[0m, in \u001b[0;36m_fit_binary\u001b[1;34m(estimator, X, y, classes)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     82\u001b[0m     estimator \u001b[39m=\u001b[39m clone(estimator)\n\u001b[1;32m---> 83\u001b[0m     estimator\u001b[39m.\u001b[39;49mfit(X, y)\n\u001b[0;32m     84\u001b[0m \u001b[39mreturn\u001b[39;00m estimator\n",
      "File \u001b[1;32mc:\\Users\\sucho\\anaconda3\\envs\\py39\\lib\\site-packages\\sklearn\\svm\\_base.py:173\u001b[0m, in \u001b[0;36mBaseLibSVM.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    171\u001b[0m     check_consistent_length(X, y)\n\u001b[0;32m    172\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 173\u001b[0m     X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m    174\u001b[0m         X,\n\u001b[0;32m    175\u001b[0m         y,\n\u001b[0;32m    176\u001b[0m         dtype\u001b[39m=\u001b[39;49mnp\u001b[39m.\u001b[39;49mfloat64,\n\u001b[0;32m    177\u001b[0m         order\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mC\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    178\u001b[0m         accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    179\u001b[0m         accept_large_sparse\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    180\u001b[0m     )\n\u001b[0;32m    182\u001b[0m y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_targets(y)\n\u001b[0;32m    184\u001b[0m sample_weight \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(\n\u001b[0;32m    185\u001b[0m     [] \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m sample_weight, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat64\n\u001b[0;32m    186\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\sucho\\anaconda3\\envs\\py39\\lib\\site-packages\\sklearn\\base.py:596\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    594\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[0;32m    595\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 596\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[0;32m    597\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    599\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\sucho\\anaconda3\\envs\\py39\\lib\\site-packages\\sklearn\\utils\\validation.py:1092\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1074\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[0;32m   1075\u001b[0m     X,\n\u001b[0;32m   1076\u001b[0m     accept_sparse\u001b[39m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1087\u001b[0m     input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1088\u001b[0m )\n\u001b[0;32m   1090\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric, estimator\u001b[39m=\u001b[39mestimator)\n\u001b[1;32m-> 1092\u001b[0m check_consistent_length(X, y)\n\u001b[0;32m   1094\u001b[0m \u001b[39mreturn\u001b[39;00m X, y\n",
      "File \u001b[1;32mc:\\Users\\sucho\\anaconda3\\envs\\py39\\lib\\site-packages\\sklearn\\utils\\validation.py:387\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    385\u001b[0m uniques \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(lengths)\n\u001b[0;32m    386\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 387\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    388\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    389\u001b[0m         \u001b[39m%\u001b[39m [\u001b[39mint\u001b[39m(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lengths]\n\u001b[0;32m    390\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [5220, 20]"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Load the value categories from the JSON file\n",
    "with open('value-categories.json') as f:\n",
    "  value_categories = json.load(f)\n",
    "\n",
    "# Create a dictionary of words and phrases for each value category\n",
    "value_dict = {}\n",
    "for category, subcategories in value_categories.items():\n",
    "  words = []\n",
    "  for subcategory, definitions in subcategories.items():\n",
    "    for definition in definitions:\n",
    "      words += definition.split()\n",
    "  value_dict[category] = set(words)\n",
    "\n",
    "# Load the training data\n",
    "arguments = pd.read_csv('arguments-training.tsv', sep='\\t')\n",
    "labels = pd.read_csv('labels-training.tsv', sep='\\t')\n",
    "\n",
    "# Preprocess the text data\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "arguments_tfidf = tfidf.fit_transform(arguments['Premise'])\n",
    "\n",
    "# Create a binary indicator for each value category\n",
    "mlb = MultiLabelBinarizer()\n",
    "labels_bin = mlb.fit_transform(labels.iloc[:,1:])\n",
    "\n",
    "# Identify the values being expressed in the arguments\n",
    "values = []\n",
    "for i in range(arguments.shape[0]):\n",
    "  argument = arguments_tfidf[i]\n",
    "  values.append([])\n",
    "  for category, word_set in value_dict.items():\n",
    "    for word in word_set:\n",
    "      if word in tfidf.vocabulary_:\n",
    "        word_idx = tfidf.vocabulary_[word]\n",
    "        if argument[0, word_idx] > 0:\n",
    "          values[-1].append(category)\n",
    "          break\n",
    "\n",
    "# Create a feature vector representing the values in each argument\n",
    "features = mlb.transform(values)\n",
    "\n",
    "# Train a classifier to classify the arguments according to their values\n",
    "classifier = OneVsRestClassifier(SVC(gamma='auto'))\n",
    "classifier.fit(features, labels_bin)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "arguments_test = pd.read_csv('arguments-test.tsv', sep='\\t')\n",
    "arguments_tfidf_test = tfidf.transform(arguments_test['Premise'])\n",
    "labels_test = pd.read_csv('labels-test.tsv', sep='\\t')\n",
    "labels_bin_test = mlb.transform(labels_test.iloc[:,1:])\n",
    "predictions = classifier.predict(arguments_tfidf_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "precision = precision_score(labels_bin_test, predictions, average='micro')\n",
    "recall = recall_score(labels_bin_test, predictions, average='micro')\n",
    "f1 = f1_score(labels_bin_test, predictions, average='micro')\n",
    "\n",
    "# Print evaluation metrics\n",
    "print('Precision: ', precision)\n",
    "print('Recall: ', recall)\n",
    "print('F1-score: ', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # create a list of words in the input text\n",
    "    input_words = input_text.split()\n",
    "\n",
    "    # create a list of words in the value category\n",
    "    category_words = category.split()\n",
    "\n",
    "    # compute the cosine similarity score between the vectors of the words in the input text and the value category\n",
    "    input_vector = fits_like_a_glove(input_words)\n",
    "    category_vector = fits_like_a_glove(category_words)\n",
    "    input_vector_norm = np.linalg.norm(input_vector)\n",
    "    category_vector_norm = np.linalg.norm(category_vector)\n",
    "    if input_vector_norm > 0 and category_vector_norm > 0:\n",
    "        cosine_similarity = cosine_similarity(input_vector, category_vector)[0][0]\n",
    "    else:\n",
    "        cosine_similarity = 0\n",
    "\n",
    "    return cosine_similarity\n",
    "\n",
    "# create an empty list to store the cosine similarity scores\n",
    "cosine_similarities = []\n",
    "\n",
    "# compute the cosine similarity score for each input text and value category\n",
    "for input_text in training_data['Cleaned_BOW']:\n",
    "    cosine_similarities.append([compute_cosine_similarity(input_text, category) for category in all_categories])\n",
    "\n",
    "# add the cosine similarity scores as a column in the feature matrix\n",
    "arguments_training['cosine_similarity'] = cosine_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of all value categories and subcategories\n",
    "all_categories = []\n",
    "for category, subcategories in value_categories.items():\n",
    "    all_categories.append(category)\n",
    "    for subcategory, _ in subcategories.items():\n",
    "        all_categories.append(subcategory)\n",
    "\n",
    "# load the arguments and labels data from the tsv files\n",
    "arguments_data = {}\n",
    "with open('arguments-training.tsv') as f:\n",
    "    for line in f:\n",
    "        argument_id, conclusion, stance, premise = line.strip().split('\\t')\n",
    "        arguments_data[argument_id] = {\n",
    "            'premise': premise,\n",
    "            'conclusion': conclusion,\n",
    "            'stance': stance,\n",
    "            'categories': []\n",
    "        }\n",
    "\n",
    "with open('labels-training.tsv') as f:\n",
    "    for line in f:\n",
    "        argument_id, *labels = line.strip().split('\\t')\n",
    "        arguments_data[argument_id]['categories'] = [\n",
    "            category\n",
    "            for category, label in zip(all_categories, labels)\n",
    "            if label == '1'\n",
    "        ]\n",
    "\n",
    "# create a list of all arguments and their corresponding categories and subcategories\n",
    "arguments = []\n",
    "categories = []\n",
    "for argument_id, argument_data in arguments_data.items():\n",
    "    premise = argument_data['premise']\n",
    "    conclusion = argument_data['conclusion']\n",
    "    stance = argument_data['stance']\n",
    "\n",
    "    # create the argument text by concatenating the premiseand conclusion\n",
    "argument_text = premise + ' ' + conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a binary feature for each value category and subcategory\n",
    "argument_categories = []\n",
    "for category in all_categories:\n",
    "    if category in argument_data['categories']:\n",
    "        argument_categories.append(1)\n",
    "    else:\n",
    "        argument_categories.append(0)\n",
    "\n",
    "# add the argument text and categories to the list\n",
    "arguments.append(argument_text)\n",
    "categories.append(argument_categories)\n",
    "\n",
    "# create a CountVectorizer to convert the argument text to numeric features\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(arguments)\n",
    "\n",
    "# combine the numeric features with the binary category features\n",
    "X_with_categories = np.hstack((X.todense(), np.array(categories)))\n",
    "\n",
    "# split the data into train, test, and validation sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_with_categories, categories, test_size=0.2)\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
    "\n",
    "# # print the dimensions of the train, test, and validation sets\n",
    "# print(f'X_train: {X_train.shape}')\n",
    "# print(f'X_test: {X_test.shape}')\n",
    "# print(f'X_val: {X_val.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(predicted_labels, true_labels, which_label=1):\n",
    "    \"\"\"\n",
    "    F1 score is the harmonic mean of precision and recall\n",
    "    \"\"\"\n",
    "    P = precision(predicted_labels, true_labels, which_label=which_label)\n",
    "    R = recall(predicted_labels, true_labels, which_label=which_label)\n",
    "    if P and R:\n",
    "        return 2*P*R/(P+R)\n",
    "    else:\n",
    "        return 0.\n",
    "\n",
    "def avg_f1_score(predicted_labels: List[int], true_labels: List[int], classes: List[int]):\n",
    "    \"\"\"\n",
    "    Calculate the f1-score for each class and return the average of it\n",
    "\n",
    "    :return: float\n",
    "    \"\"\"\n",
    "    f1_scores = []\n",
    "    for c in classes:\n",
    "        f1_scores.append(f1_score(predicted_labels, true_labels, which_label=c))\n",
    "    return sum(f1_scores)/len(f1_scores)\n",
    "    # raise NotImplementedError(\"Calculate the f1-score for each class and return the average of it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "With n_samples=1, test_size=0.5 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [34], line 11\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39m# # drop Argument ID column in labels_training\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39m# labels_training_new = labels_training.drop(labels_training.columns[0], axis=1)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m \n\u001b[0;32m      9\u001b[0m \u001b[39m# split the input texts and labels into train, val, and test sets\u001b[39;00m\n\u001b[0;32m     10\u001b[0m X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m train_test_split(X_with_categories, categories, test_size\u001b[39m=\u001b[39m\u001b[39m.2\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m X_train, X_val, y_train, y_val \u001b[39m=\u001b[39m train_test_split(X_train, y_train, test_size\u001b[39m=\u001b[39;49m\u001b[39m.5\u001b[39;49m, random_state\u001b[39m=\u001b[39;49m\u001b[39m42\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\sucho\\anaconda3\\envs\\py39\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2448\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2445\u001b[0m arrays \u001b[39m=\u001b[39m indexable(\u001b[39m*\u001b[39marrays)\n\u001b[0;32m   2447\u001b[0m n_samples \u001b[39m=\u001b[39m _num_samples(arrays[\u001b[39m0\u001b[39m])\n\u001b[1;32m-> 2448\u001b[0m n_train, n_test \u001b[39m=\u001b[39m _validate_shuffle_split(\n\u001b[0;32m   2449\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[39m=\u001b[39;49m\u001b[39m0.25\u001b[39;49m\n\u001b[0;32m   2450\u001b[0m )\n\u001b[0;32m   2452\u001b[0m \u001b[39mif\u001b[39;00m shuffle \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[0;32m   2453\u001b[0m     \u001b[39mif\u001b[39;00m stratify \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\sucho\\anaconda3\\envs\\py39\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2126\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[1;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[0;32m   2123\u001b[0m n_train, n_test \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(n_train), \u001b[39mint\u001b[39m(n_test)\n\u001b[0;32m   2125\u001b[0m \u001b[39mif\u001b[39;00m n_train \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m-> 2126\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   2127\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWith n_samples=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, test_size=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m and train_size=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2128\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2129\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39maforementioned parameters.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(n_samples, test_size, train_size)\n\u001b[0;32m   2130\u001b[0m     )\n\u001b[0;32m   2132\u001b[0m \u001b[39mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[1;31mValueError\u001b[0m: With n_samples=1, test_size=0.5 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # drop Argument ID column in labels_training\n",
    "# labels_training_new = labels_training.drop(labels_training.columns[0], axis=1)\n",
    "\n",
    "# # drop Argument ID column in training_data\n",
    "# training_data_new = training_data.drop(training_data.columns[0], axis=1)\n",
    "\n",
    "# split the input texts and labels into train, val, and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_with_categories, categories, test_size=.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4176, 522, 522, 4176, 522, 522, 5220, 5220)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(X_val), len(X_test), len(y_train), len(y_val), len(y_test), len(training_data), len(labels_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the dimensions of the train, test, and validation sets\n",
    "print(f'X_train: {X_train.shape}')\n",
    "print(f'X_test: {X_test.shape}')\n",
    "print(f'X_val: {X_val.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'We should open up three-child policy to encourage childbirth.'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [116], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Train an SVM classifier on the training data\u001b[39;00m\n\u001b[0;32m      2\u001b[0m clf \u001b[39m=\u001b[39m SVC()\n\u001b[1;32m----> 3\u001b[0m clf\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n",
      "File \u001b[1;32mc:\\Users\\sucho\\anaconda3\\envs\\py39\\lib\\site-packages\\sklearn\\svm\\_base.py:173\u001b[0m, in \u001b[0;36mBaseLibSVM.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    171\u001b[0m     check_consistent_length(X, y)\n\u001b[0;32m    172\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 173\u001b[0m     X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m    174\u001b[0m         X,\n\u001b[0;32m    175\u001b[0m         y,\n\u001b[0;32m    176\u001b[0m         dtype\u001b[39m=\u001b[39;49mnp\u001b[39m.\u001b[39;49mfloat64,\n\u001b[0;32m    177\u001b[0m         order\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mC\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    178\u001b[0m         accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    179\u001b[0m         accept_large_sparse\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    180\u001b[0m     )\n\u001b[0;32m    182\u001b[0m y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_targets(y)\n\u001b[0;32m    184\u001b[0m sample_weight \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(\n\u001b[0;32m    185\u001b[0m     [] \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m sample_weight, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat64\n\u001b[0;32m    186\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\sucho\\anaconda3\\envs\\py39\\lib\\site-packages\\sklearn\\base.py:596\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    594\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[0;32m    595\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 596\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[0;32m    597\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    599\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\sucho\\anaconda3\\envs\\py39\\lib\\site-packages\\sklearn\\utils\\validation.py:1074\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1069\u001b[0m         estimator_name \u001b[39m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1070\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1071\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m requires y to be passed, but the target y is None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1072\u001b[0m     )\n\u001b[1;32m-> 1074\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[0;32m   1075\u001b[0m     X,\n\u001b[0;32m   1076\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49maccept_sparse,\n\u001b[0;32m   1077\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39;49maccept_large_sparse,\n\u001b[0;32m   1078\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m   1079\u001b[0m     order\u001b[39m=\u001b[39;49morder,\n\u001b[0;32m   1080\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m   1081\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite,\n\u001b[0;32m   1082\u001b[0m     ensure_2d\u001b[39m=\u001b[39;49mensure_2d,\n\u001b[0;32m   1083\u001b[0m     allow_nd\u001b[39m=\u001b[39;49mallow_nd,\n\u001b[0;32m   1084\u001b[0m     ensure_min_samples\u001b[39m=\u001b[39;49mensure_min_samples,\n\u001b[0;32m   1085\u001b[0m     ensure_min_features\u001b[39m=\u001b[39;49mensure_min_features,\n\u001b[0;32m   1086\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[0;32m   1087\u001b[0m     input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1088\u001b[0m )\n\u001b[0;32m   1090\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric, estimator\u001b[39m=\u001b[39mestimator)\n\u001b[0;32m   1092\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32mc:\\Users\\sucho\\anaconda3\\envs\\py39\\lib\\site-packages\\sklearn\\utils\\validation.py:856\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    854\u001b[0m         array \u001b[39m=\u001b[39m array\u001b[39m.\u001b[39mastype(dtype, casting\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39munsafe\u001b[39m\u001b[39m\"\u001b[39m, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    855\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 856\u001b[0m         array \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49masarray(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[0;32m    857\u001b[0m \u001b[39mexcept\u001b[39;00m ComplexWarning \u001b[39mas\u001b[39;00m complex_warning:\n\u001b[0;32m    858\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    859\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mComplex data not supported\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[0;32m    860\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sucho\\anaconda3\\envs\\py39\\lib\\site-packages\\pandas\\core\\generic.py:2069\u001b[0m, in \u001b[0;36mNDFrame.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   2068\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__array__\u001b[39m(\u001b[39mself\u001b[39m, dtype: npt\u001b[39m.\u001b[39mDTypeLike \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[1;32m-> 2069\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49masarray(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_values, dtype\u001b[39m=\u001b[39;49mdtype)\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'We should open up three-child policy to encourage childbirth.'"
     ]
    }
   ],
   "source": [
    "# Train an SVM classifier on the training data\n",
    "clf = SVC()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Test the classifier on the test data\n",
    "# predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, The experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:Argument ID: object, Conclusion: object, Stance: object, Premise: object, Cleaned_Premise: object, Cleaned_Conclusion: object, Cleaned_BOW: object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [109], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m early_stopping_rounds \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[0;32m     14\u001b[0m \u001b[39m# create a DMatrix from the training data\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m dtrain \u001b[39m=\u001b[39m xgb\u001b[39m.\u001b[39;49mDMatrix(X_train, y_train\u001b[39m.\u001b[39;49mto_numpy(), enable_categorical\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     17\u001b[0m model \u001b[39m=\u001b[39m xgb\u001b[39m.\u001b[39mcv(params, dtrain, num_boost_round\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m, early_stopping_rounds\u001b[39m=\u001b[39mearly_stopping_rounds,\n\u001b[0;32m     18\u001b[0m                verbose_eval\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     20\u001b[0m \u001b[39m# get the number of boosting rounds that resulted in the best validation performance\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sucho\\anaconda3\\envs\\py39\\lib\\site-packages\\xgboost\\core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[0;32m    619\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[1;32m--> 620\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\sucho\\anaconda3\\envs\\py39\\lib\\site-packages\\xgboost\\core.py:743\u001b[0m, in \u001b[0;36mDMatrix.__init__\u001b[1;34m(self, data, label, weight, base_margin, missing, silent, feature_names, feature_types, nthread, group, qid, label_lower_bound, label_upper_bound, feature_weights, enable_categorical)\u001b[0m\n\u001b[0;32m    740\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    741\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m--> 743\u001b[0m handle, feature_names, feature_types \u001b[39m=\u001b[39m dispatch_data_backend(\n\u001b[0;32m    744\u001b[0m     data,\n\u001b[0;32m    745\u001b[0m     missing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmissing,\n\u001b[0;32m    746\u001b[0m     threads\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnthread,\n\u001b[0;32m    747\u001b[0m     feature_names\u001b[39m=\u001b[39;49mfeature_names,\n\u001b[0;32m    748\u001b[0m     feature_types\u001b[39m=\u001b[39;49mfeature_types,\n\u001b[0;32m    749\u001b[0m     enable_categorical\u001b[39m=\u001b[39;49menable_categorical,\n\u001b[0;32m    750\u001b[0m )\n\u001b[0;32m    751\u001b[0m \u001b[39massert\u001b[39;00m handle \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    752\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle \u001b[39m=\u001b[39m handle\n",
      "File \u001b[1;32mc:\\Users\\sucho\\anaconda3\\envs\\py39\\lib\\site-packages\\xgboost\\data.py:957\u001b[0m, in \u001b[0;36mdispatch_data_backend\u001b[1;34m(data, missing, threads, feature_names, feature_types, enable_categorical)\u001b[0m\n\u001b[0;32m    955\u001b[0m     \u001b[39mreturn\u001b[39;00m _from_tuple(data, missing, threads, feature_names, feature_types)\n\u001b[0;32m    956\u001b[0m \u001b[39mif\u001b[39;00m _is_pandas_df(data):\n\u001b[1;32m--> 957\u001b[0m     \u001b[39mreturn\u001b[39;00m _from_pandas_df(data, enable_categorical, missing, threads,\n\u001b[0;32m    958\u001b[0m                            feature_names, feature_types)\n\u001b[0;32m    959\u001b[0m \u001b[39mif\u001b[39;00m _is_pandas_series(data):\n\u001b[0;32m    960\u001b[0m     \u001b[39mreturn\u001b[39;00m _from_pandas_series(\n\u001b[0;32m    961\u001b[0m         data, missing, threads, enable_categorical, feature_names, feature_types\n\u001b[0;32m    962\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\sucho\\anaconda3\\envs\\py39\\lib\\site-packages\\xgboost\\data.py:404\u001b[0m, in \u001b[0;36m_from_pandas_df\u001b[1;34m(data, enable_categorical, missing, nthread, feature_names, feature_types)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_from_pandas_df\u001b[39m(\n\u001b[0;32m    397\u001b[0m     data: DataFrame,\n\u001b[0;32m    398\u001b[0m     enable_categorical: \u001b[39mbool\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    402\u001b[0m     feature_types: Optional[FeatureTypes],\n\u001b[0;32m    403\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DispatchedDataBackendReturnType:\n\u001b[1;32m--> 404\u001b[0m     data, feature_names, feature_types \u001b[39m=\u001b[39m _transform_pandas_df(\n\u001b[0;32m    405\u001b[0m         data, enable_categorical, feature_names, feature_types\n\u001b[0;32m    406\u001b[0m     )\n\u001b[0;32m    407\u001b[0m     \u001b[39mreturn\u001b[39;00m _from_numpy_array(data, missing, nthread, feature_names, feature_types)\n",
      "File \u001b[1;32mc:\\Users\\sucho\\anaconda3\\envs\\py39\\lib\\site-packages\\xgboost\\data.py:378\u001b[0m, in \u001b[0;36m_transform_pandas_df\u001b[1;34m(data, enable_categorical, feature_names, feature_types, meta, meta_type)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapi\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtypes\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m    367\u001b[0m     is_sparse,\n\u001b[0;32m    368\u001b[0m     is_categorical_dtype,\n\u001b[0;32m    369\u001b[0m )\n\u001b[0;32m    371\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mall\u001b[39m(\n\u001b[0;32m    372\u001b[0m     dtype\u001b[39m.\u001b[39mname \u001b[39min\u001b[39;00m _pandas_dtype_mapper\n\u001b[0;32m    373\u001b[0m     \u001b[39mor\u001b[39;00m is_sparse(dtype)\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    376\u001b[0m     \u001b[39mfor\u001b[39;00m dtype \u001b[39min\u001b[39;00m data\u001b[39m.\u001b[39mdtypes\n\u001b[0;32m    377\u001b[0m ):\n\u001b[1;32m--> 378\u001b[0m     _invalid_dataframe_dtype(data)\n\u001b[0;32m    380\u001b[0m feature_names, feature_types \u001b[39m=\u001b[39m _pandas_feature_info(\n\u001b[0;32m    381\u001b[0m     data, meta, feature_names, feature_types, enable_categorical\n\u001b[0;32m    382\u001b[0m )\n\u001b[0;32m    384\u001b[0m transformed \u001b[39m=\u001b[39m _pandas_cat_null(data)\n",
      "File \u001b[1;32mc:\\Users\\sucho\\anaconda3\\envs\\py39\\lib\\site-packages\\xgboost\\data.py:270\u001b[0m, in \u001b[0;36m_invalid_dataframe_dtype\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    268\u001b[0m type_err \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mDataFrame.dtypes for data must be int, float, bool or category.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    269\u001b[0m msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\"\"\u001b[39m\u001b[39m{\u001b[39;00mtype_err\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00m_ENABLE_CAT_ERR\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00merr\u001b[39m}\u001b[39;00m\u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m--> 270\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n",
      "\u001b[1;31mValueError\u001b[0m: DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, The experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:Argument ID: object, Conclusion: object, Stance: object, Premise: object, Cleaned_Premise: object, Cleaned_Conclusion: object, Cleaned_BOW: object"
     ]
    }
   ],
   "source": [
    "# import the required modules\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# create an XGBoost model\n",
    "model = xgb.XGBClassifier()\n",
    "\n",
    "# specify the parameters for the model\n",
    "params = {'max_depth': 3, 'eta': 0.1, 'num_class': 20}\n",
    "\n",
    "# define a callback function for early stopping\n",
    "early_stopping_rounds = 10\n",
    "\n",
    "# create a DMatrix from the training data\n",
    "dtrain = xgb.DMatrix(X_train, y_train.to_numpy(), enable_categorical=True)\n",
    "\n",
    "model = xgb.cv(params, dtrain, num_boost_round=1000, early_stopping_rounds=early_stopping_rounds,\n",
    "               verbose_eval=True)\n",
    "\n",
    "# get the number of boosting rounds that resulted in the best validation performance\n",
    "best_iteration = model.best_iteration\n",
    "\n",
    "# train a new model using the best number of boosting rounds\n",
    "final_model = xgb.train(params, dtrain, num_boost_round=best_iteration)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Argument ID                   object\n",
      "Self-direction: thought        int64\n",
      "Self-direction: action         int64\n",
      "Stimulation                    int64\n",
      "Hedonism                       int64\n",
      "Achievement                    int64\n",
      "Power: dominance               int64\n",
      "Power: resources               int64\n",
      "Face                           int64\n",
      "Security: personal             int64\n",
      "Security: societal             int64\n",
      "Tradition                      int64\n",
      "Conformity: rules              int64\n",
      "Conformity: interpersonal      int64\n",
      "Humility                       int64\n",
      "Benevolence: caring            int64\n",
      "Benevolence: dependability     int64\n",
      "Universalism: concern          int64\n",
      "Universalism: nature           int64\n",
      "Universalism: tolerance        int64\n",
      "Universalism: objectivity      int64\n",
      "Conclusion                    object\n",
      "Stance                        object\n",
      "Premise                       object\n",
      "Cleaned_Premise               object\n",
      "Cleaned_Conclusion            object\n",
      "Cleaned_BOW                   object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(X_train.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Create a new Empath lexicon\n",
    "lexicon = Empath()\n",
    "\n",
    "categories = list(value_categories.keys())\n",
    "# values = list(value_categories.values())\n",
    "\n",
    "# Create a dictionary of values for each category\n",
    "values = {}\n",
    "for category, value_map in value_categories.items():\n",
    "    values[category] = list(value_map.keys())\n",
    "\n",
    "# Create the lexicon\n",
    "for category, values in values.items():\n",
    "    lexicon.create_category(category, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the arguments and labels data from the tsv files\n",
    "arguments_data = {}\n",
    "with open('arguments-training.tsv') as f:\n",
    "    for line in f:\n",
    "        argument_id, conclusion, stance, premise = line.strip().split('\\t')\n",
    "        arguments_data[argument_id] = {\n",
    "            'premise': premise,\n",
    "            'conclusion': conclusion,\n",
    "            'stance': stance,\n",
    "            'categories': []\n",
    "        }\n",
    "\n",
    "with open('labels-training.tsv') as f:\n",
    "    for line in f:\n",
    "        argument_id, *labels = line.strip().split('\\t')\n",
    "        arguments_data[argument_id]['categories'] = [\n",
    "            category\n",
    "            for category, label in zip(all_categories, labels)\n",
    "            if label == '1'\n",
    "        ]\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# load the value-categories.json file\n",
    "with open('value-categories.json') as json_file:\n",
    "    value_categories = json.load(json_file)\n",
    "\n",
    "# create a list of all value categories and subcategories\n",
    "all_categories = []\n",
    "for category, subcategories in value_categories.items():\n",
    "    all_categories.append(category)\n",
    "    for subcategory, _ in subcategories.items():\n",
    "        all_categories.append(subcategory)\n",
    "\n",
    "# create a list of all arguments and their corresponding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Empath' object has no attribute '_lexicon'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [155], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Print the categories and their values\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[39mfor\u001b[39;00m category \u001b[39min\u001b[39;00m lexicon\u001b[39m.\u001b[39;49m_lexicon:\n\u001b[0;32m      3\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mcategory\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mlexicon\u001b[39m.\u001b[39m_lexicon[category]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Empath' object has no attribute '_lexicon'"
     ]
    }
   ],
   "source": [
    "# Print the categories and their values\n",
    "for category in lexicon._lexicon:\n",
    "    print(f\"{category}: {lexicon._lexicon[category]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('Be logical', ['being better by the numbers but not by gut feeling', 'fostering a rational way of thinking', 'promoting focus and consistency', 'promoting the rational analysis of circumstances', 'promoting the scientific method']), ('Have an objective view', ['fostering to seek the truth', 'fostering to take on a neutral perspective', 'promoting to form an unbiased opinion', 'providing people with the means to make informed decisions', 'weighing all pros and cons'])])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Self-direction: thought': {'Be creative': ['allowing for more creativity or imagination',\n",
       "   'being more creative',\n",
       "   'fostering creativity',\n",
       "   'promoting imagination'],\n",
       "  'Be curious': ['being the more interesting option',\n",
       "   'fostering curiosity',\n",
       "   'making people more keen to learn',\n",
       "   'promoting discoveries',\n",
       "   'sparking interest'],\n",
       "  'Have freedom of thought': ['allowing people to figure things out on their own',\n",
       "   'allowing people to make up their mind',\n",
       "   'resulting in less censorship',\n",
       "   \"resulting in less influence on people's thoughts\"]},\n",
       " 'Self-direction: action': {'Be choosing own goals': ['allowing people to choose what is best for them',\n",
       "   'allowing people to decide on their life',\n",
       "   'allowing people to follow their dreams'],\n",
       "  'Be independent': ['allowing people to plan on their own',\n",
       "   'resulting in fewer times people have to ask for consent'],\n",
       "  'Have freedom of action': ['allowing people to be self-determined',\n",
       "   'allowing people to do things even though this may hurt them in the long run',\n",
       "   'resulting in more times people can do what they want'],\n",
       "  'Have privacy': ['allowing for private spaces',\n",
       "   'allowing for time alone',\n",
       "   'resulting in less surveillance',\n",
       "   'resulting in more control on what to disclose and to whom']},\n",
       " 'Stimulation': {'Have an exciting life': ['allowing people to experience foreign places',\n",
       "   'providing perspective-changing experiences',\n",
       "   'providing special activities'],\n",
       "  'Have a varied life': ['allowing people to change parts of their life',\n",
       "   'allowing people to move flat easily',\n",
       "   'promoting local clubs (sports, ...)',\n",
       "   'providing many activities'],\n",
       "  'Be daring': ['allowing for risky actions',\n",
       "   'allowing to take risks',\n",
       "   'being more risky',\n",
       "   'fostering risk-taking']},\n",
       " 'Hedonism': {'Have pleasure': ['making life enjoyable',\n",
       "   'providing leisure',\n",
       "   'providing opportunities to have fun',\n",
       "   'providing sensuous gratification']},\n",
       " 'Achievement': {'Be ambitious': ['allowing for ambitions',\n",
       "   'being more ambitious',\n",
       "   'fostering ambition',\n",
       "   'providing incentives for the difficult climb up the social ladder'],\n",
       "  'Have success': ['allowing for success',\n",
       "   'being more successful',\n",
       "   'recognizing achievements'],\n",
       "  'Be capable': ['allowing to acquire competence in certain tasks',\n",
       "   'being more effective',\n",
       "   'resulting in a higher effectivity',\n",
       "   'showing competence in solving tasks'],\n",
       "  'Be intellectual': ['allowing to acquire high cognitive skills',\n",
       "   'being more reflective',\n",
       "   'resulting in more reflective behavior',\n",
       "   'showing intelligence'],\n",
       "  'Be courageous': ['being more courageous',\n",
       "   'fostering courage',\n",
       "   'making people stand up for their beliefs',\n",
       "   'promoting courage',\n",
       "   'showing courage']},\n",
       " 'Power: dominance': {'Have influence': ['having more people to ask for a favor',\n",
       "   'resulting in more influence',\n",
       "   'resulting in more obligations towards the own side',\n",
       "   'resulting in more ways to control events'],\n",
       "  'Have the right to command': ['allowing experts to tell others what to do',\n",
       "   'allowing people to take command',\n",
       "   'fostering leadership',\n",
       "   'resulting in clearer hierarchies of command']},\n",
       " 'Power: resources': {'Have wealth': ['allowing people to gain wealth and material possession',\n",
       "   \"allowing to show one's wealth\",\n",
       "   'allowing to use money for power',\n",
       "   'providing people with resources to control events',\n",
       "   'resulting in financial prosperity']},\n",
       " 'Face': {'Have social recognition': ['allowing people to gain respect',\n",
       "   'avoiding humiliation',\n",
       "   'providing social recognition for actions'],\n",
       "  'Have a good reputation': ['allowing people to build up their reputation',\n",
       "   \"protecting one's public image\",\n",
       "   'spreading reputation']},\n",
       " 'Security: personal': {'Have a sense of belonging': ['allowing people to establish groups',\n",
       "   'allowing people to join groups and show their group membership',\n",
       "   'allowing group members to show they care for each other',\n",
       "   'fostering a sense of belonging',\n",
       "   'resulting in fewer people forced to leave their groups'],\n",
       "  'Have good health': ['avoiding diseases',\n",
       "   'preserving health',\n",
       "   'having physiological and mental well-being',\n",
       "   'fostering activities to stay healthy',\n",
       "   'resulting in increased health'],\n",
       "  'Have no debts': ['avoiding indebtedness',\n",
       "   'having people always return a favor',\n",
       "   'reciprocating favors'],\n",
       "  'Be neat and tidy': ['allowing to clean up',\n",
       "   'being more clean or orderly',\n",
       "   'promoting cleanliness or neatness',\n",
       "   'resulting in higher cleanliness'],\n",
       "  'Have a comfortable life': ['providing subsistence income',\n",
       "   'resulting in having no financial worries',\n",
       "   'resulting in a higher general happiness',\n",
       "   'resulting in a prosperous life']},\n",
       " 'Security: societal': {'Have a safe country': ['caring for citizens',\n",
       "   'resulting in a state that can better act on crimes',\n",
       "   'resulting in a state that can better defend its citizens',\n",
       "   'resulting in a state that takes better care of its citizens',\n",
       "   'resulting in a stronger state'],\n",
       "  'Have a stable society': ['accepting or maintaining the existing social structure',\n",
       "   'preventing chaos and disorder',\n",
       "   'promoting the social order',\n",
       "   'resulting in a country that is more stable']},\n",
       " 'Tradition': {'Be respecting traditions': [\"allowing to follow one's family's customs\",\n",
       "   'honoring traditional practices',\n",
       "   'maintaining traditional values and ways of thinking',\n",
       "   'promoting the preservation of customs'],\n",
       "  'Be holding religious faith': [\"allowing to devote one's life to their faith\",\n",
       "   'allowing the customs of a religion',\n",
       "   'being more adequate for a certain religion',\n",
       "   'promoting piety',\n",
       "   'spreading a religion']},\n",
       " 'Conformity: rules': {'Be compliant': ['abiding to laws or rules',\n",
       "   \"promoting to meet one's obligations\",\n",
       "   'recognizing people who abide to laws or rules'],\n",
       "  'Be self-disciplined': ['fostering to exercise restraint',\n",
       "   'fostering to follow rules even when no-one is watching',\n",
       "   'fostering to set rules for oneself'],\n",
       "  'Be behaving properly': ['avoiding to violate informal rules or social conventions',\n",
       "   'fostering good manners',\n",
       "   'resulting in more people minding their manners']},\n",
       " 'Conformity: interpersonal': {'Be polite': ['avoiding to upset other people',\n",
       "   'promoting to take others into account',\n",
       "   'resulting in being less annoying for others'],\n",
       "  'Be honoring elders': ['fostering that children follow their parents',\n",
       "   \"showing faith and respect towards one's elders\"]},\n",
       " 'Humility': {'Be humble': ['demoting arrogance',\n",
       "   'demoting to think one deserves more than other people',\n",
       "   'emphasizing the successful group over single persons',\n",
       "   'fostering to give back to society for the opportunities one got',\n",
       "   'fostering to not brag about what one achieved'],\n",
       "  'Have life accepted as is': ['allowing people to accept their fate',\n",
       "   \"fostering to submit to life's circumstances\",\n",
       "   'promoting satisfaction with what one has',\n",
       "   \"showing acceptance of one's own portion in life\"]},\n",
       " 'Benevolence: caring': {'Be helpful': [\"allowing to help the people in one's group\",\n",
       "   'being more helpful to those one cares for',\n",
       "   'fostering a readiness to help each other',\n",
       "   'promoting to work for the welfare of others in one group'],\n",
       "  'Be honest': ['being more honest',\n",
       "   'fostering honest ways of thinking',\n",
       "   'promoting honesty',\n",
       "   'recognizing people for their honesty',\n",
       "   'resulting in more honest social interaction'],\n",
       "  'Be forgiving': ['allowing people to forgive each other',\n",
       "   'giving people a second chance',\n",
       "   'being merciful',\n",
       "   'promoting a will to pardon others',\n",
       "   'providing paths to redemption'],\n",
       "  'Have the own family secured': ['allowing people to protect their family',\n",
       "   'promoting to have a family',\n",
       "   \"providing care for one's family\"],\n",
       "  'Be loving': ['allowing to place the well-being of others above the own well-being',\n",
       "   \"allowing to show one's affection, compassion and sympathy\",\n",
       "   'fostering close relationships',\n",
       "   'promoting self-respect and self-love as a means of care for oneself',\n",
       "   'promoting to concern oneself with the needs of dear ones']},\n",
       " 'Benevolence: dependability': {'Be responsible': ['allowing for clear responsibilities',\n",
       "   'fostering dependability',\n",
       "   'promoting reliability',\n",
       "   'resulting in more people being confident',\n",
       "   'taking responsibility'],\n",
       "  'Have loyalty towards friends': ['being a dependable and trustworthy friend',\n",
       "   'foster loyalty towards friends',\n",
       "   'promoting to give friends a full backing']},\n",
       " 'Universalism: concern': {'Have equality': ['fostering people of a lower social status',\n",
       "   'helping poorer regions of the world',\n",
       "   'providing all people with equal opportunities in life',\n",
       "   'resulting in a world were success is less determined by birth'],\n",
       "  'Be just': [\"allowing justice to be 'blind' to irrelevant aspects of a case\",\n",
       "   'fostering a sense for justice',\n",
       "   'promoting fairness in competitions',\n",
       "   'protecting the weak and vulnerable in society',\n",
       "   'resulting a world were people are less discriminated based on race, gender, ...'],\n",
       "  'Have a world at peace': ['allowing for nations to cease fire',\n",
       "   'avoiding conflicts',\n",
       "   'fostering to see peace as fragile and precious',\n",
       "   'promoting to end wars',\n",
       "   'resulting in more people caring for all of humanity']},\n",
       " 'Universalism: nature': {'Be protecting the environment': ['avoiding pollution',\n",
       "   'fostering to care for nature',\n",
       "   'promoting programs to restore nature',\n",
       "   'resulting in less damage to the ecosystem'],\n",
       "  'Have harmony with nature': ['allowing to avoid chemicals (especially in nutrition)',\n",
       "   'allowing to avoid genetically modified organisms',\n",
       "   'fostering to treat animals or plants like them having souls',\n",
       "   'promoting a life in harmony with nature',\n",
       "   'resulting in more people reflecting the consequences of their actions towards the environment'],\n",
       "  'Have a world of beauty': ['allowing people to experience art',\n",
       "   'fostering to stand in awe of nature',\n",
       "   'promoting fine arts',\n",
       "   'promoting the beauty of nature',\n",
       "   'spreading beauty']},\n",
       " 'Universalism: tolerance': {'Be broadminded': ['allowing for discussion between groups',\n",
       "   'clearing up with prejudices',\n",
       "   'fostering to listen to and understand people who are different from oneself',\n",
       "   'promoting tolerance towards all kinds of people and groups',\n",
       "   'promoting to life within a different group for some time'],\n",
       "  'Have the wisdom to accept others': ['allowing people to accept disagreements',\n",
       "   'fostering to accept people even when one disagrees with them',\n",
       "   'promoting a mature understanding of different opinions',\n",
       "   'resulting in fewer partisans or fanatics']},\n",
       " 'Universalism: objectivity': {'Be logical': ['being better by the numbers but not by gut feeling',\n",
       "   'fostering a rational way of thinking',\n",
       "   'promoting focus and consistency',\n",
       "   'promoting the rational analysis of circumstances',\n",
       "   'promoting the scientific method'],\n",
       "  'Have an objective view': ['fostering to seek the truth',\n",
       "   'fostering to take on a neutral perspective',\n",
       "   'promoting to form an unbiased opinion',\n",
       "   'providing people with the means to make informed decisions',\n",
       "   'weighing all pros and cons']}}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Self-direction: thought',\n",
       " 'Self-direction: action',\n",
       " 'Stimulation',\n",
       " 'Hedonism',\n",
       " 'Achievement',\n",
       " 'Power: dominance',\n",
       " 'Power: resources',\n",
       " 'Face',\n",
       " 'Security: personal',\n",
       " 'Security: societal',\n",
       " 'Tradition',\n",
       " 'Conformity: rules',\n",
       " 'Conformity: interpersonal',\n",
       " 'Humility',\n",
       " 'Benevolence: caring',\n",
       " 'Benevolence: dependability',\n",
       " 'Universalism: concern',\n",
       " 'Universalism: nature',\n",
       " 'Universalism: tolerance',\n",
       " 'Universalism: objectivity']"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to search for phrases in text\n",
    "def search_phrases(text, phrases):\n",
    "    # Iterate through phrases and check if each is present in the text\n",
    "    for phrase in phrases:\n",
    "        if phrase in text:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Function to extract features for each argument\n",
    "def extract_features(text):\n",
    "    # Initialize dictionary to store features\n",
    "    features = {}\n",
    "\n",
    "    # Iterate through the value categories and extract associated words\n",
    "    for value, words in value_categories.items():\n",
    "        # Use the search_phrases() function to check if the words are present in the text\n",
    "        features[value] = search_phrases(text, words)\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Extract features for each argument in the training data\n",
    "train_features = []\n",
    "for text in train['Cleaned_BOW']:\n",
    "    train_features.append(extract_features(text))\n",
    "\n",
    "# Extract features for each argument in the validation data\n",
    "val_features = []\n",
    "for text in val['Cleaned_BOW']:\n",
    "    val_features.append(extract_features(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incorporate value-categories in feature extraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=0.5, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=None, feature_types=None, gamma=0, gpu_id=-1,\n",
       "              grow_policy=&#x27;depthwise&#x27;, importance_type=None,\n",
       "              interaction_constraints=&#x27;&#x27;, learning_rate=0.300000012,\n",
       "              max_bin=256, max_cat_threshold=64, max_cat_to_onehot=4,\n",
       "              max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n",
       "              missing=nan, monotone_constraints=&#x27;()&#x27;, n_estimators=100,\n",
       "              n_jobs=0, num_parallel_tree=1, predictor=&#x27;auto&#x27;, random_state=0, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=0.5, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=None, feature_types=None, gamma=0, gpu_id=-1,\n",
       "              grow_policy=&#x27;depthwise&#x27;, importance_type=None,\n",
       "              interaction_constraints=&#x27;&#x27;, learning_rate=0.300000012,\n",
       "              max_bin=256, max_cat_threshold=64, max_cat_to_onehot=4,\n",
       "              max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n",
       "              missing=nan, monotone_constraints=&#x27;()&#x27;, n_estimators=100,\n",
       "              n_jobs=0, num_parallel_tree=1, predictor=&#x27;auto&#x27;, random_state=0, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=None, feature_types=None, gamma=0, gpu_id=-1,\n",
       "              grow_policy='depthwise', importance_type=None,\n",
       "              interaction_constraints='', learning_rate=0.300000012,\n",
       "              max_bin=256, max_cat_threshold=64, max_cat_to_onehot=4,\n",
       "              max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n",
       "              missing=nan, monotone_constraints='()', n_estimators=100,\n",
       "              n_jobs=0, num_parallel_tree=1, predictor='auto', random_state=0, ...)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dictionary mapping each human value to a list of associated words\n",
    "value_dict = {value: words for value, words in value_categories.items()}\n",
    "\n",
    "# Extract features which check for human value being present in Cleaned BOW\n",
    "for i in value_dict:\n",
    "    train[i] = train['Cleaned_BOW'].str.contains(\"|\".join(value_dict[i]), regex=True)\n",
    "\n",
    "# Concatenate additional features with the word vectors to create a single input vector for each argument\n",
    "XGB_X = np.concatenate((train_vectors.toarray(), train.drop(['Argument ID', 'Premise', 'Conclusion', 'Stance', 'Cleaned_Premise', 'Cleaned_Conclusion', 'Cleaned_BOW'], axis=1).to_numpy()), axis=1)\n",
    "XGB_X_val = np.concatenate((val_vectors.toarray(), val.drop(['Argument ID', 'Premise', 'Conclusion', 'Stance', 'Cleaned_Premise', 'Cleaned_Conclusion', 'Cleaned_BOW'], axis=1).to_numpy()), axis=1)\n",
    "\n",
    "# Train XGB model with the input vectors and value labels as target variable\n",
    "labels = labels_training.values[:, 1:]\n",
    "XGB_model = xgboost.XGBClassifier()\n",
    "XGB_model.fit(XGB_X, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1020, 1020)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XGB_X.shape[1], XGB_X_val.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cheeck F1 and accuracy on the validation set for xgb model\n",
    "val_preds = XGB_model.predict(XGB_X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_labels = val.values[:, 1:]\n",
    "\n",
    "\n",
    "\n",
    "label_f1s = []\n",
    "for i in range(val_labels.shape[1]):\n",
    "    label_f1 = f1_score(val_labels[:, i], val_preds[:, i])\n",
    "    label_f1s.append(label_f1)\n",
    "\n",
    "# Calculate average F1-score across all labels\n",
    "avg_f1 = np.mean(label_f1s)\n",
    "print(\"Average F1-score: {:.4f}\".format(avg_f1))\n",
    "\n",
    "# Calculate overall accuracy\n",
    "acc = (val_preds == val_labels).all(axis=1).mean()\n",
    "print(\"Overall accuracy: {:.4f}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    4176\n",
       "Name: Stimulation, dtype: int64"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get value counts for each column in train\n",
    "train['Stimulation'].value_counts()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a OneVsRestClassifier with a linear SVC\n",
    "clf = OneVsRestClassifier(SVC(kernel='linear', probability=True))\n",
    "\n",
    "# fit the model to the training data\n",
    "clf.fit(train_vectors, train['Label'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGB Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an XGBoost classifier\n",
    "from xgboost import XGBClassifier\n",
    "xgb = XGBClassifier()\n",
    "\n",
    "# fit the model to the training data\n",
    "xgb.fit(train_vectors, train['Label'])\n",
    "\n",
    "# create a random forest classifier\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# fit the model to the training data\n",
    "rf.fit(train_vectors, train['Label'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add a dense layer with 128 units and ReLU activation\n",
    "model.add(Dense(128, activation=\"relu\"))\n",
    "\n",
    "# Add a dense layer with 20 units (one for each value category) and sigmoid activation\n",
    "model.add(Dense(20, activation=\"sigmoid\"))\n",
    "\n",
    "# Compile the model with binary cross-entropy loss and Adam optimizer\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\n",
    "\n",
    "# Fit the model on the training data\n",
    "model.fit(X, labels, epochs=10, batch_size=32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras model 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the text of the arguments into a sequence of word vectors\n",
    "X = sequence.pad_sequences(arguments[\"text\"], maxlen=100)\n",
    "\n",
    "# Create a sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add an embedding layer with 100-dimensional word vectors and input length of 100\n",
    "model.add(Embedding(vocabulary_size, 100, input_length=100))\n",
    "\n",
    "# Add an LSTM layer with 128 units\n",
    "model.add(LSTM(128))\n",
    "\n",
    "# Add a dense layer with 20 units (one for each value category) and sigmoid activation\n",
    "model.add(Dense(20, activation=\"sigmoid\"))\n",
    "\n",
    "# Compile the model with binary cross-entropy loss and Adam optimizer\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\n",
    "\n",
    "# Fit the model on the training data\n",
    "model.fit(X, labels, epochs=10, batch_size=32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "408ff3118643ffe4a17d68811b8c1004a4d886e4bbf4052e7d08910a4f0a7aa3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
