{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from bert import optimization\n",
    "from bert import tokenization\n",
    "from datetime import datetime\n",
    "import en_core_web_trf\n",
    "import xgboost as xgb\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import *\n",
    "from keras.layers import *\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.initializers import Constant\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tf_slim as slim\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost\n",
    "\n",
    "import collections\n",
    "import transformers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "from empath import Empath\n",
    "from typing import List\n",
    "\n",
    "import re\n",
    "import os\n",
    "import bs4\n",
    "import bert\n",
    "import spacy\n",
    "import string\n",
    "import keras\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets as pd.DataFrames\n",
    "arguments_training = pd.read_csv('arguments-training.tsv', sep='\\t')\n",
    "labels_training = pd.read_csv('labels-training.tsv', sep='\\t')\n",
    "\n",
    "# Load value-categories.json\n",
    "with open('value-categories.json') as f:\n",
    "    value_categories = json.load(f)\n",
    "\n",
    "# this is a list of all value categories and subcategories\n",
    "all_categories = []\n",
    "for category, subcategories in value_categories.items():\n",
    "    all_categories.append(category)\n",
    "\n",
    "# arguments including labels\n",
    "arguments_data = {}\n",
    "with open('arguments-training.tsv') as f:\n",
    "    for line in f:\n",
    "        argument_id, conclusion, stance, premise = line.strip().split('\\t')\n",
    "        arguments_data[argument_id] = {\n",
    "            'premise': premise,\n",
    "            'conclusion': conclusion,\n",
    "            'stance': stance,\n",
    "            'categories': []\n",
    "        }\n",
    "\n",
    "# labels to arguments\n",
    "with open('labels-training.tsv') as f:\n",
    "    for line in f:\n",
    "        argument_id, *labels = line.strip().split('\\t')\n",
    "        arguments_data[argument_id]['categories'] = [\n",
    "            category\n",
    "            for category, label in zip(all_categories, labels)\n",
    "            if label == '1'\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pd.DataFrame that contains the premise, conclusion, stance, and categories for each argument\n",
    "arguments_data = pd.DataFrame({\n",
    "    'premise': arguments_training['Premise'],\n",
    "    'conclusion': arguments_training['Conclusion'],\n",
    "    'stance': arguments_training['Stance'],\n",
    "    'categories': labels_training.drop(columns=['Argument ID']).apply(lambda x: x.astype(bool).index[x].tolist(), axis=1)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roomba(item):\n",
    "    '''\n",
    "    input: string\n",
    "    output: list of cleaned words\n",
    "\n",
    "    '''\n",
    "    soup = bs4.BeautifulSoup(item, 'html.parser')\n",
    "    text = soup.get_text()\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "    text = re.sub(r'\\[.*\\]\\(.*\\)', '', text)\n",
    "\n",
    "    # remove '[removed]' and '[deleted]'\n",
    "    text = re.sub(r'\\[.*\\]', '', text)\n",
    "\n",
    "    # remove non utf-8 characters\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "\n",
    "    # lowercase \n",
    "    text = text.lower()\n",
    "\n",
    "    # remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    # remove leading and trailing spaces\n",
    "    text = text.strip()\n",
    "\n",
    "    # remove urls\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "\n",
    "    # remove stopwords\n",
    "    text = [word for word in text.split() if word not in stopwords]\n",
    "    text = [lemmatizer.lemmatize(word) for word in text]\n",
    "\n",
    "    if text == '':\n",
    "        return False\n",
    "    else:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# create a tokenizer with NLTK\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "tokenizer = word_tokenize\n",
    "\n",
    "# create a lemmatizer with NLTK\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sucho\\anaconda3\\envs\\py39\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# run the roomba function on the arguments_training dataframe append a 'cleaned' column\n",
    "arguments_training['Cleaned_Premise'] = arguments_training['Premise'].apply(roomba)\n",
    "arguments_training['Cleaned_Conclusion'] = arguments_training['Conclusion'].apply(roomba)\n",
    "\n",
    "arguments_training['Cleaned_Premise'] = arguments_training['Premise'].apply(roomba)\n",
    "arguments_training['Cleaned_Conclusion'] = arguments_training['Conclusion'].apply(roomba)\n",
    "\n",
    "# combine 'conclusion', 'premise', and 'stance' into one column, then run roomba function\n",
    "arguments_training['Cleaned_BOW'] = arguments_training['Premise'] + ' ' + arguments_training['Conclusion'] + ' ' + arguments_training['Stance']\n",
    "training_data = pd.merge(labels_training, arguments_training, on='Argument ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import embedding models\n",
    "path_to_gloVe_file = \"glove.6B.200d.txt\"\n",
    "\n",
    "# dictionary of words to embeddings\n",
    "embeddings_index = {}\n",
    "\n",
    "with open(path_to_gloVe_file, encoding='utf8') as f:\n",
    "  for line in f:\n",
    "    word, coefs = line.split(maxsplit = 1)\n",
    "    coefs = np.fromstring(coefs, \"f\", sep = \" \")\n",
    "    embeddings_index[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find argument with longest length of words\n",
    "max_tokens = arguments_training['Cleaned_BOW'].str.split().str.len().max()\n",
    "data = arguments_training['Cleaned_BOW']\n",
    "\n",
    "# This text vectorizer indexs our vocabulary based on the train sampl\n",
    "vectorizer = TextVectorization(max_tokens = max_tokens, output_sequence_length = 100, split = 'whitespace')\n",
    "data_tensor = tf.data.Dataset.from_tensor_slices(data).batch(32)\n",
    "vectorizer.adapt(data_tensor)\n",
    "\n",
    "# map unique words to integers\n",
    "vocabulary = vectorizer.get_vocabulary()\n",
    "index = dict(zip(vocabulary, range(len(vocabulary))))\n",
    "len_vocabulary = len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding coverage:  97.92 %\n",
      "Captured words:  141\n",
      "Missed words:  3\n"
     ]
    }
   ],
   "source": [
    "# create embedding matrix\n",
    "embedding_dim = 200\n",
    "\n",
    "# going to count hits and misses\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "\n",
    "embedding_matrix = np.zeros((len_vocabulary, embedding_dim))\n",
    "for word, i in index.items():\n",
    "  embedding_vector = embeddings_index.get(word)\n",
    "  if embedding_vector is not None:\n",
    "    embedding_matrix[i] = embedding_vector\n",
    "    hits+=1\n",
    "  else:\n",
    "    misses+=1\n",
    "\n",
    "embedding_layer = Embedding(len_vocabulary, embedding_dim, embeddings_initializer=Constant(embedding_matrix), trainable = False)\n",
    "\n",
    "\n",
    "print(\"Embedding coverage: \", round((hits/(hits+misses))*100,2),\"%\")\n",
    "print(\"Captured words: \", hits)\n",
    "print(\"Missed words: \", misses)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create token list for each value category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['allowing', 'for', 'more', 'creativity', 'or', 'imagination', 'being', 'more', 'creative', 'fostering']\n",
      "['allowing', 'creativity', 'imagination', 'creative', 'fostering', 'creativity', 'promoting', 'imagination', 'interesting', 'option']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "tokens = {}\n",
    "\n",
    "for category, values in value_categories.items():\n",
    "  tokens[category] = []\n",
    "  for examples in values.values():\n",
    "    tokens[category].extend([token for example in examples for token in example.split()])\n",
    "\n",
    "print(tokens['Self-direction: thought'][:10])\n",
    "\n",
    "for category in tokens.keys():\n",
    "  tokens[category] = [token.lower() for token in tokens[category]]\n",
    "  tokens[category] = [token for token in tokens[category] if token not in stop_words]\n",
    "  tokens[category] = [lemmatizer.lemmatize(token) for token in tokens[category]]\n",
    "\n",
    "\n",
    "print(tokens['Self-direction: thought'][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fits_like_a_glove(words):\n",
    "    word_vectors = np.zeros((len(words), embedding_dim))\n",
    "    for i, word in enumerate(words):\n",
    "        if word in vocabulary:\n",
    "            word_vectors[i] = embeddings_index[word]\n",
    "\n",
    "    input_vector = np.sum(word_vectors, axis=0)\n",
    "    return input_vector"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fits_like_a_glove(words):\n",
    "    word_vectors = np.zeros((len(words), embedding_dim))\n",
    "    for i, word in enumerate(words):\n",
    "        if word in embeddings_index:\n",
    "            word_vectors[i] = embeddings_index[word]\n",
    "        else:\n",
    "            word_vectors[i] = np.random.uniform(-1, 1, embedding_dim)\n",
    "\n",
    "    input_vector = np.sum(word_vectors, axis=0)\n",
    "    return input_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_cosine(input_vector, category_tokens):\n",
    "    #categoryString=\" \"\n",
    "    #categoryString = categoryString.join(category_tokens)\n",
    "    #print(categoryString)\n",
    "    category_vector = fits_like_a_glove(category_tokens)\n",
    "    #print(input_vector)\n",
    "    #print(category_vector)\n",
    "    similarity = cosine_similarity([input_vector], [category_vector])\n",
    "\n",
    "    return similarity[0][0]\n",
    "\n",
    "def predict_category(input_text):\n",
    "    similarity_scores = {}\n",
    "\n",
    "    input_vector = fits_like_a_glove(input_text)\n",
    "\n",
    "    for category in value_category:\n",
    "        similarity_scores[category] = calc_cosine(input_vector, category)\n",
    "    return max(similarity_scores, key=similarity_scores.get)\n",
    "\n",
    "cosine_similarities = []\n",
    "\n",
    "for input_text in training_data['Cleaned_BOW']:\n",
    "    input_vector = fits_like_a_glove(input_text.split())\n",
    "    for category in all_categories:\n",
    "        value = [calc_cosine(input_vector, tokens[category])]\n",
    "        #print(value)\n",
    "        cosine_similarities.append(value)\n",
    "        \n",
    "value_category_list = list(value_categories.keys())\n",
    "\n",
    "\n",
    "cos_sims = [item for sublist in cosine_similarities for item in sublist]\n",
    "\n",
    "# output the cosine similarity score for each respective value category in a new column in arguments_training\n",
    "for category in value_category_list:\n",
    "    i = value_category_list.index(category)\n",
    "    arguments_training[f'Cosine_Similarity_{category}'] = np.reshape(cos_sims,(int(len(cos_sims)/len(value_category_list)), len(value_category_list)))[:,i]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find an overlap of tokens between input text and category values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_features = {}\n",
    "\n",
    "def contains_tokens(input_tokens, tokens):\n",
    "  input_tokens = set(input_tokens)\n",
    "  tokens = set(tokens)\n",
    "  return len(input_tokens.intersection(tokens)) / len(input_tokens)\n",
    "\n",
    "binary_features = {}\n",
    "\n",
    "for category in tokens.keys():\n",
    "  binary_features[category] = arguments_training['Cleaned_BOW'].apply(lambda x: contains_tokens(x.split(), tokens[category]))\n",
    "\n",
    "for category, feature in binary_features.items():\n",
    "  arguments_training[f'contains_{category}'] = feature"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add additional features (word count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the length of the premise and word count as additional columns in the arguments_training dataframe\n",
    "arguments_training['premise_length'] = arguments_training['Premise'].apply(len)\n",
    "\n",
    "# normalize the length of the premise and word count columns\n",
    "arguments_training['premise_length'] = arguments_training['premise_length'] / arguments_training['premise_length'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments_training.to_csv('arguments_training.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split test, train, and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4176, 522, 522, 4176, 522, 522)"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop index columns in arguments_training\n",
    "feature_values = arguments_training.drop(['Argument ID', 'Conclusion', 'Stance', 'Premise', 'Cleaned_Premise',\n",
    "       'Cleaned_Conclusion', 'Cleaned_BOW', 'word_count'], axis=1)\n",
    "\n",
    "labels = labels_training.drop(['Argument ID'], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_values, labels, test_size=0.2, random_state=42)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
    "\n",
    "len(X_train), len(X_val), len(X_test), len(y_train), len(y_val), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = {\n",
    "'Self-direction: thought': ['contains_Self-direction: thought', 'Cosine_Similarity_Self-direction: thought', 'premise_length'],\n",
    "'Self-direction: action': ['contains_Self-direction: action', 'Cosine_Similarity_Self-direction: action', 'premise_length'],\n",
    "'Stimulation': ['contains_Stimulation', 'Cosine_Similarity_Stimulation', 'premise_length'],\n",
    "'Hedonism': ['contains_Hedonism', 'Cosine_Similarity_Hedonism', 'premise_length'],\n",
    "'Achievement': ['contains_Achievement', 'Cosine_Similarity_Achievement', 'premise_length'],\n",
    "'Power: dominance': ['contains_Power: dominance', 'Cosine_Similarity_Power: dominance', 'premise_length'],\n",
    "'Power: resources': ['contains_Power: resources', 'Cosine_Similarity_Power: resources', 'premise_length'],\n",
    "'Face': ['contains_Face', 'Cosine_Similarity_Face', 'premise_length'],\n",
    "'Security: personal': ['contains_Security: personal', 'Cosine_Similarity_Security: personal', 'premise_length'],\n",
    "'Security: societal': ['contains_Security: societal', 'Cosine_Similarity_Security: societal', 'premise_length'],\n",
    "'Tradition': ['contains_Tradition', 'Cosine_Similarity_Tradition', 'premise_length'],\n",
    "'Conformity: rules': ['contains_Conformity: rules', 'Cosine_Similarity_Conformity: rules', 'premise_length'],\n",
    "'Conformity: interpersonal': ['contains_Conformity: interpersonal', 'Cosine_Similarity_Conformity: interpersonal', 'premise_length'],\n",
    "'Humility': ['contains_Humility', 'Cosine_Similarity_Humility', 'premise_length'],\n",
    "'Benevolence: caring': ['contains_Benevolence: caring', 'Cosine_Similarity_Benevolence: caring', 'premise_length'],\n",
    "'Benevolence: dependability': ['contains_Benevolence: dependability', 'Cosine_Similarity_Benevolence: dependability', 'premise_length'],\n",
    "'Universalism: concern': ['contains_Universalism: concern', 'Cosine_Similarity_Universalism: concern', 'premise_length'],\n",
    "'Universalism: nature': ['contains_Universalism: nature', 'Cosine_Similarity_Universalism: nature', 'premise_length'],\n",
    "'Universalism: tolerance': ['contains_Universalism: tolerance', 'Cosine_Similarity_Universalism: tolerance', 'premise_length'],\n",
    "'Universalism: objectivity': ['contains_Universalism: objectivity', 'Cosine_Similarity_Universalism: objectivity', 'premise_length']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KERAS_MODELS = {}\n",
    "\n",
    "for category, features in feature_columns.items():\n",
    "    print(f'Training model for {category}')\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=len(features), activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(X_train[features], y_train[category], epochs=10, batch_size=10, verbose=0)\n",
    "    KERAS_MODELS[category] = model\n",
    "    \n",
    "predictions = {}\n",
    "\n",
    "for category, features in feature_columns.items():\n",
    "    print(f'Predicting labels for {category}')\n",
    "    predictions[category] = KERAS_MODELS[category].predict(X_val[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KERAS = {}\n",
    "\n",
    "for category, features in feature_columns.items():\n",
    "    KERAS[category] = [f1_score(y_val[category], predictions[category].round()), accuracy_score(y_val[category], predictions[category].round()), recall_score(y_val[category], predictions[category].round()), precision_score(y_val[category], predictions[category].round())]\n",
    "\n",
    "\n",
    "KERAS_scores = pd.DataFrame.from_dict(KERAS, orient='index', columns=['F1', 'Accuracy', 'Recall', 'Precision'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Linear_SVC_scores = pd.DataFrame(columns=['F1-Micro', 'F1-Macro', 'Accuracy', 'Recall', 'Precision'])\n",
    "for label, columns in feature_columns.items():\n",
    "    # Select the relevant columns from X_train and X_val\n",
    "    X_train_subset = X_train[columns]\n",
    "    X_val_subset = X_val[columns]\n",
    "\n",
    "    classifier = LinearSVC(random_state=42)\n",
    "    classifier.fit(X_train_subset, y_train[label])\n",
    "    y_pred = classifier.predict(X_val_subset)\n",
    "\n",
    "\n",
    "    f1_micro = f1_score(y_val[label], y_pred, average='micro')\n",
    "    f1_macro = f1_score(y_val[label], y_pred, average='macro')\n",
    "    accuracy = accuracy_score(y_val[label], y_pred)\n",
    "    recall = recall_score(y_val[label], y_pred, average='macro')\n",
    "    precision = precision_score(y_val[label], y_pred, average='macro')\n",
    "\n",
    "    # print('Label: {} - F1 score: {}'.format(label, f1))\n",
    "    # print('Label: {} - Accuracy: {}'.format(label, accuracy))\n",
    "    # print('Label: {} - Recall: {}'.format(label, recall))\n",
    "    # print('Label: {} - Precision: {}'.format(label, precision))\n",
    "\n",
    "    # add a row to the dataframe for each label\n",
    "    Linear_SVC_scores.loc[label] = [f1_micro, f1_macro, accuracy, recall, precision]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGB_scores = pd.DataFrame(columns=['F1-Micro', 'F1-Macro', 'Accuracy', 'Recall', 'Precision'])\n",
    "for label, columns in feature_columns.items():\n",
    "    # Select the relevant columns from X_train and X_val\n",
    "    X_train_subset = X_train[columns]\n",
    "    X_val_subset = X_val[columns]\n",
    "\n",
    "    classifier = XGBClassifier(random_state=42)\n",
    "    classifier.fit(X_train_subset, y_train[label])\n",
    "    y_pred = classifier.predict(X_val_subset)\n",
    "\n",
    "\n",
    "    f1_micro = f1_score(y_val[label], y_pred, average='micro')\n",
    "    f1_macro = f1_score(y_val[label], y_pred, average='macro')\n",
    "    accuracy = accuracy_score(y_val[label], y_pred)\n",
    "    recall = recall_score(y_val[label], y_pred, average='macro')\n",
    "    precision = precision_score(y_val[label], y_pred, average='macro')\n",
    "\n",
    "    # print('Label: {} - F1 score: {}'.format(label, f1))\n",
    "    # print('Label: {} - Accuracy: {}'.format(label, accuracy))\n",
    "    # print('Label: {} - Recall: {}'.format(label, recall))\n",
    "    # print('Label: {} - Precision: {}'.format(label, precision))\n",
    "\n",
    "    # add a row to the dataframe for each label\n",
    "    XGB_scores.loc[label] = [f1_micro, f1_macro, accuracy, recall, precision]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.concat([Linear_SVC_scores, XGB_scores, KERAS_scores], axis=1, keys=['LinearSVC', 'XGB', 'Keras'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sucho\\AppData\\Local\\Temp\\ipykernel_15684\\1107829678.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  scores['LinearSVC']['Winner'] = scores['LinearSVC'].idxmax(axis=1)\n",
      "C:\\Users\\sucho\\AppData\\Local\\Temp\\ipykernel_15684\\1107829678.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  scores['XGB']['Winner'] = scores['XGB'].idxmax(axis=1)\n",
      "C:\\Users\\sucho\\AppData\\Local\\Temp\\ipykernel_15684\\1107829678.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  scores['Keras']['Winner'] = scores['Keras'].idxmax(axis=1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"5\" halign=\"left\">LinearSVC</th>\n",
       "      <th colspan=\"5\" halign=\"left\">XGB</th>\n",
       "      <th colspan=\"4\" halign=\"left\">Keras</th>\n",
       "      <th>Winner</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>F1-Micro</th>\n",
       "      <th>F1-Macro</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1-Micro</th>\n",
       "      <th>F1-Macro</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Self-direction: thought</th>\n",
       "      <td>0.823755</td>\n",
       "      <td>0.451681</td>\n",
       "      <td>0.823755</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.411877</td>\n",
       "      <td>0.808429</td>\n",
       "      <td>0.474997</td>\n",
       "      <td>0.808429</td>\n",
       "      <td>0.503514</td>\n",
       "      <td>0.519544</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.823755</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>(LinearSVC, F1-Micro)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Self-direction: action</th>\n",
       "      <td>0.747126</td>\n",
       "      <td>0.448891</td>\n",
       "      <td>0.747126</td>\n",
       "      <td>0.511111</td>\n",
       "      <td>0.872832</td>\n",
       "      <td>0.718391</td>\n",
       "      <td>0.521188</td>\n",
       "      <td>0.718391</td>\n",
       "      <td>0.532730</td>\n",
       "      <td>0.569967</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.747126</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>(Keras, Precision)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stimulation</th>\n",
       "      <td>0.959770</td>\n",
       "      <td>0.489736</td>\n",
       "      <td>0.959770</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.479885</td>\n",
       "      <td>0.955939</td>\n",
       "      <td>0.488737</td>\n",
       "      <td>0.955939</td>\n",
       "      <td>0.498004</td>\n",
       "      <td>0.479808</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.959770</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>(LinearSVC, F1-Micro)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hedonism</th>\n",
       "      <td>0.969349</td>\n",
       "      <td>0.492218</td>\n",
       "      <td>0.969349</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.484674</td>\n",
       "      <td>0.959770</td>\n",
       "      <td>0.489736</td>\n",
       "      <td>0.959770</td>\n",
       "      <td>0.495059</td>\n",
       "      <td>0.484526</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.969349</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>(LinearSVC, F1-Micro)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Achievement</th>\n",
       "      <td>0.695402</td>\n",
       "      <td>0.410169</td>\n",
       "      <td>0.695402</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.347701</td>\n",
       "      <td>0.672414</td>\n",
       "      <td>0.502605</td>\n",
       "      <td>0.672414</td>\n",
       "      <td>0.524118</td>\n",
       "      <td>0.551724</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.695402</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>(LinearSVC, F1-Micro)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Power: dominance</th>\n",
       "      <td>0.908046</td>\n",
       "      <td>0.475904</td>\n",
       "      <td>0.908046</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.454023</td>\n",
       "      <td>0.900383</td>\n",
       "      <td>0.473790</td>\n",
       "      <td>0.900383</td>\n",
       "      <td>0.495781</td>\n",
       "      <td>0.453668</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.908046</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>(LinearSVC, F1-Micro)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Power: resources</th>\n",
       "      <td>0.875479</td>\n",
       "      <td>0.466803</td>\n",
       "      <td>0.875479</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.437739</td>\n",
       "      <td>0.877395</td>\n",
       "      <td>0.522634</td>\n",
       "      <td>0.877395</td>\n",
       "      <td>0.527487</td>\n",
       "      <td>0.726491</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.875479</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>(XGB, F1-Micro)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Face</th>\n",
       "      <td>0.925287</td>\n",
       "      <td>0.480597</td>\n",
       "      <td>0.925287</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.462644</td>\n",
       "      <td>0.921456</td>\n",
       "      <td>0.479561</td>\n",
       "      <td>0.921456</td>\n",
       "      <td>0.497930</td>\n",
       "      <td>0.462500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.925287</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>(LinearSVC, F1-Micro)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Security: personal</th>\n",
       "      <td>0.641762</td>\n",
       "      <td>0.463314</td>\n",
       "      <td>0.641762</td>\n",
       "      <td>0.540305</td>\n",
       "      <td>0.787698</td>\n",
       "      <td>0.590038</td>\n",
       "      <td>0.536096</td>\n",
       "      <td>0.590038</td>\n",
       "      <td>0.540976</td>\n",
       "      <td>0.549394</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.616858</td>\n",
       "      <td>0.059113</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>(LinearSVC, Precision)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Security: societal</th>\n",
       "      <td>0.689655</td>\n",
       "      <td>0.408163</td>\n",
       "      <td>0.689655</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>0.641762</td>\n",
       "      <td>0.492638</td>\n",
       "      <td>0.641762</td>\n",
       "      <td>0.509414</td>\n",
       "      <td>0.516022</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.689655</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>(LinearSVC, F1-Micro)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tradition</th>\n",
       "      <td>0.886973</td>\n",
       "      <td>0.470051</td>\n",
       "      <td>0.886973</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.443487</td>\n",
       "      <td>0.871648</td>\n",
       "      <td>0.480134</td>\n",
       "      <td>0.871648</td>\n",
       "      <td>0.498755</td>\n",
       "      <td>0.493359</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.886973</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>(LinearSVC, F1-Micro)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Conformity: rules</th>\n",
       "      <td>0.785441</td>\n",
       "      <td>0.439914</td>\n",
       "      <td>0.785441</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.392720</td>\n",
       "      <td>0.752874</td>\n",
       "      <td>0.477364</td>\n",
       "      <td>0.752874</td>\n",
       "      <td>0.501982</td>\n",
       "      <td>0.505979</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.785441</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>(LinearSVC, F1-Micro)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Conformity: interpersonal</th>\n",
       "      <td>0.963602</td>\n",
       "      <td>0.490732</td>\n",
       "      <td>0.963602</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.481801</td>\n",
       "      <td>0.961686</td>\n",
       "      <td>0.490234</td>\n",
       "      <td>0.961686</td>\n",
       "      <td>0.499006</td>\n",
       "      <td>0.481766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.963602</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>(LinearSVC, F1-Micro)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Humility</th>\n",
       "      <td>0.885057</td>\n",
       "      <td>0.469512</td>\n",
       "      <td>0.885057</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.442529</td>\n",
       "      <td>0.881226</td>\n",
       "      <td>0.468432</td>\n",
       "      <td>0.881226</td>\n",
       "      <td>0.497835</td>\n",
       "      <td>0.442308</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.885057</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>(LinearSVC, F1-Micro)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Benevolence: caring</th>\n",
       "      <td>0.724138</td>\n",
       "      <td>0.457380</td>\n",
       "      <td>0.724138</td>\n",
       "      <td>0.516408</td>\n",
       "      <td>0.695906</td>\n",
       "      <td>0.683908</td>\n",
       "      <td>0.499230</td>\n",
       "      <td>0.683908</td>\n",
       "      <td>0.517361</td>\n",
       "      <td>0.535560</td>\n",
       "      <td>0.039474</td>\n",
       "      <td>0.720307</td>\n",
       "      <td>0.020408</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>(LinearSVC, F1-Micro)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Benevolence: dependability</th>\n",
       "      <td>0.827586</td>\n",
       "      <td>0.452830</td>\n",
       "      <td>0.827586</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.413793</td>\n",
       "      <td>0.816092</td>\n",
       "      <td>0.478456</td>\n",
       "      <td>0.816092</td>\n",
       "      <td>0.506250</td>\n",
       "      <td>0.539706</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.827586</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>(LinearSVC, F1-Micro)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Universalism: concern</th>\n",
       "      <td>0.622605</td>\n",
       "      <td>0.402111</td>\n",
       "      <td>0.622605</td>\n",
       "      <td>0.508447</td>\n",
       "      <td>0.710445</td>\n",
       "      <td>0.580460</td>\n",
       "      <td>0.498524</td>\n",
       "      <td>0.580460</td>\n",
       "      <td>0.514068</td>\n",
       "      <td>0.519859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.616858</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>(LinearSVC, Precision)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Universalism: nature</th>\n",
       "      <td>0.948276</td>\n",
       "      <td>0.486726</td>\n",
       "      <td>0.948276</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.474138</td>\n",
       "      <td>0.942529</td>\n",
       "      <td>0.516428</td>\n",
       "      <td>0.942529</td>\n",
       "      <td>0.514478</td>\n",
       "      <td>0.574855</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.948276</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>(LinearSVC, F1-Micro)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Universalism: tolerance</th>\n",
       "      <td>0.862069</td>\n",
       "      <td>0.462963</td>\n",
       "      <td>0.862069</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.431034</td>\n",
       "      <td>0.852490</td>\n",
       "      <td>0.496082</td>\n",
       "      <td>0.852490</td>\n",
       "      <td>0.511944</td>\n",
       "      <td>0.568849</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.862069</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>(LinearSVC, F1-Micro)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Universalism: objectivity</th>\n",
       "      <td>0.816092</td>\n",
       "      <td>0.449367</td>\n",
       "      <td>0.816092</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.408046</td>\n",
       "      <td>0.804598</td>\n",
       "      <td>0.489922</td>\n",
       "      <td>0.804598</td>\n",
       "      <td>0.513131</td>\n",
       "      <td>0.566329</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.816092</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>(LinearSVC, F1-Micro)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           LinearSVC                                          \\\n",
       "                            F1-Micro  F1-Macro  Accuracy    Recall Precision   \n",
       "Self-direction: thought     0.823755  0.451681  0.823755  0.500000  0.411877   \n",
       "Self-direction: action      0.747126  0.448891  0.747126  0.511111  0.872832   \n",
       "Stimulation                 0.959770  0.489736  0.959770  0.500000  0.479885   \n",
       "Hedonism                    0.969349  0.492218  0.969349  0.500000  0.484674   \n",
       "Achievement                 0.695402  0.410169  0.695402  0.500000  0.347701   \n",
       "Power: dominance            0.908046  0.475904  0.908046  0.500000  0.454023   \n",
       "Power: resources            0.875479  0.466803  0.875479  0.500000  0.437739   \n",
       "Face                        0.925287  0.480597  0.925287  0.500000  0.462644   \n",
       "Security: personal          0.641762  0.463314  0.641762  0.540305  0.787698   \n",
       "Security: societal          0.689655  0.408163  0.689655  0.500000  0.344828   \n",
       "Tradition                   0.886973  0.470051  0.886973  0.500000  0.443487   \n",
       "Conformity: rules           0.785441  0.439914  0.785441  0.500000  0.392720   \n",
       "Conformity: interpersonal   0.963602  0.490732  0.963602  0.500000  0.481801   \n",
       "Humility                    0.885057  0.469512  0.885057  0.500000  0.442529   \n",
       "Benevolence: caring         0.724138  0.457380  0.724138  0.516408  0.695906   \n",
       "Benevolence: dependability  0.827586  0.452830  0.827586  0.500000  0.413793   \n",
       "Universalism: concern       0.622605  0.402111  0.622605  0.508447  0.710445   \n",
       "Universalism: nature        0.948276  0.486726  0.948276  0.500000  0.474138   \n",
       "Universalism: tolerance     0.862069  0.462963  0.862069  0.500000  0.431034   \n",
       "Universalism: objectivity   0.816092  0.449367  0.816092  0.500000  0.408046   \n",
       "\n",
       "                                 XGB                                          \\\n",
       "                            F1-Micro  F1-Macro  Accuracy    Recall Precision   \n",
       "Self-direction: thought     0.808429  0.474997  0.808429  0.503514  0.519544   \n",
       "Self-direction: action      0.718391  0.521188  0.718391  0.532730  0.569967   \n",
       "Stimulation                 0.955939  0.488737  0.955939  0.498004  0.479808   \n",
       "Hedonism                    0.959770  0.489736  0.959770  0.495059  0.484526   \n",
       "Achievement                 0.672414  0.502605  0.672414  0.524118  0.551724   \n",
       "Power: dominance            0.900383  0.473790  0.900383  0.495781  0.453668   \n",
       "Power: resources            0.877395  0.522634  0.877395  0.527487  0.726491   \n",
       "Face                        0.921456  0.479561  0.921456  0.497930  0.462500   \n",
       "Security: personal          0.590038  0.536096  0.590038  0.540976  0.549394   \n",
       "Security: societal          0.641762  0.492638  0.641762  0.509414  0.516022   \n",
       "Tradition                   0.871648  0.480134  0.871648  0.498755  0.493359   \n",
       "Conformity: rules           0.752874  0.477364  0.752874  0.501982  0.505979   \n",
       "Conformity: interpersonal   0.961686  0.490234  0.961686  0.499006  0.481766   \n",
       "Humility                    0.881226  0.468432  0.881226  0.497835  0.442308   \n",
       "Benevolence: caring         0.683908  0.499230  0.683908  0.517361  0.535560   \n",
       "Benevolence: dependability  0.816092  0.478456  0.816092  0.506250  0.539706   \n",
       "Universalism: concern       0.580460  0.498524  0.580460  0.514068  0.519859   \n",
       "Universalism: nature        0.942529  0.516428  0.942529  0.514478  0.574855   \n",
       "Universalism: tolerance     0.852490  0.496082  0.852490  0.511944  0.568849   \n",
       "Universalism: objectivity   0.804598  0.489922  0.804598  0.513131  0.566329   \n",
       "\n",
       "                               Keras                                \\\n",
       "                                  F1  Accuracy    Recall Precision   \n",
       "Self-direction: thought     0.000000  0.823755  0.000000  0.000000   \n",
       "Self-direction: action      0.043478  0.747126  0.022222  1.000000   \n",
       "Stimulation                 0.000000  0.959770  0.000000  0.000000   \n",
       "Hedonism                    0.000000  0.969349  0.000000  0.000000   \n",
       "Achievement                 0.000000  0.695402  0.000000  0.000000   \n",
       "Power: dominance            0.000000  0.908046  0.000000  0.000000   \n",
       "Power: resources            0.000000  0.875479  0.000000  0.000000   \n",
       "Face                        0.000000  0.925287  0.000000  0.000000   \n",
       "Security: personal          0.107143  0.616858  0.059113  0.571429   \n",
       "Security: societal          0.000000  0.689655  0.000000  0.000000   \n",
       "Tradition                   0.000000  0.886973  0.000000  0.000000   \n",
       "Conformity: rules           0.000000  0.785441  0.000000  0.000000   \n",
       "Conformity: interpersonal   0.000000  0.963602  0.000000  0.000000   \n",
       "Humility                    0.000000  0.885057  0.000000  0.000000   \n",
       "Benevolence: caring         0.039474  0.720307  0.020408  0.600000   \n",
       "Benevolence: dependability  0.000000  0.827586  0.000000  0.000000   \n",
       "Universalism: concern       0.000000  0.616858  0.000000  0.000000   \n",
       "Universalism: nature        0.000000  0.948276  0.000000  0.000000   \n",
       "Universalism: tolerance     0.000000  0.862069  0.000000  0.000000   \n",
       "Universalism: objectivity   0.000000  0.816092  0.000000  0.000000   \n",
       "\n",
       "                                            Winner  \n",
       "                                                    \n",
       "Self-direction: thought      (LinearSVC, F1-Micro)  \n",
       "Self-direction: action          (Keras, Precision)  \n",
       "Stimulation                  (LinearSVC, F1-Micro)  \n",
       "Hedonism                     (LinearSVC, F1-Micro)  \n",
       "Achievement                  (LinearSVC, F1-Micro)  \n",
       "Power: dominance             (LinearSVC, F1-Micro)  \n",
       "Power: resources                   (XGB, F1-Micro)  \n",
       "Face                         (LinearSVC, F1-Micro)  \n",
       "Security: personal          (LinearSVC, Precision)  \n",
       "Security: societal           (LinearSVC, F1-Micro)  \n",
       "Tradition                    (LinearSVC, F1-Micro)  \n",
       "Conformity: rules            (LinearSVC, F1-Micro)  \n",
       "Conformity: interpersonal    (LinearSVC, F1-Micro)  \n",
       "Humility                     (LinearSVC, F1-Micro)  \n",
       "Benevolence: caring          (LinearSVC, F1-Micro)  \n",
       "Benevolence: dependability   (LinearSVC, F1-Micro)  \n",
       "Universalism: concern       (LinearSVC, Precision)  \n",
       "Universalism: nature         (LinearSVC, F1-Micro)  \n",
       "Universalism: tolerance      (LinearSVC, F1-Micro)  \n",
       "Universalism: objectivity    (LinearSVC, F1-Micro)  "
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores['LinearSVC']['Winner'] = scores['LinearSVC'].idxmax(axis=1)\n",
    "scores['XGB']['Winner'] = scores['XGB'].idxmax(axis=1)\n",
    "scores['Keras']['Winner'] = scores['Keras'].idxmax(axis=1)\n",
    "\n",
    "scores['Winner'] = scores.idxmax(axis=1)\n",
    "\n",
    "scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final = pd.concat([X_train, X_val])\n",
    "y_train_final = pd.concat([y_train, y_val])\n",
    "\n",
    "final_run_scores = pd.DataFrame(columns=['F1-Micro', 'F1-Macro', 'Accuracy', 'Recall', 'Precision'])\n",
    "for label, columns in feature_columns.items():\n",
    "    X_train_subset = X_train_final[columns]\n",
    "    X_val_subset = X_test[columns]\n",
    "\n",
    "    classifier = LinearSVC(random_state=42)\n",
    "    classifier.fit(X_train_subset, y_train_final[label])\n",
    "    y_pred = classifier.predict(X_val_subset)\n",
    "\n",
    "    f1_micro = f1_score(y_test[label], y_pred, average='micro')\n",
    "    f1_macro = f1_score(y_test[label], y_pred, average='macro')\n",
    "    accuracy = accuracy_score(y_test[label], y_pred)\n",
    "    recall = recall_score(y_test[label], y_pred, average='macro')\n",
    "    precision = precision_score(y_test[label], y_pred, average='macro')\n",
    "\n",
    "    final_run_scores.loc[label] = [f1_micro, f1_macro, accuracy, recall, precision]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1-Micro</th>\n",
       "      <th>F1-Macro</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Self-direction: thought</th>\n",
       "      <td>0.795019</td>\n",
       "      <td>0.442903</td>\n",
       "      <td>0.795019</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.397510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Self-direction: action</th>\n",
       "      <td>0.752874</td>\n",
       "      <td>0.436987</td>\n",
       "      <td>0.752874</td>\n",
       "      <td>0.503846</td>\n",
       "      <td>0.876200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stimulation</th>\n",
       "      <td>0.963602</td>\n",
       "      <td>0.490732</td>\n",
       "      <td>0.963602</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.481801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hedonism</th>\n",
       "      <td>0.978927</td>\n",
       "      <td>0.494676</td>\n",
       "      <td>0.978927</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.489464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Achievement</th>\n",
       "      <td>0.731801</td>\n",
       "      <td>0.422566</td>\n",
       "      <td>0.731801</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.365900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Power: dominance</th>\n",
       "      <td>0.909962</td>\n",
       "      <td>0.476429</td>\n",
       "      <td>0.909962</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.454981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Power: resources</th>\n",
       "      <td>0.911877</td>\n",
       "      <td>0.476954</td>\n",
       "      <td>0.911877</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.455939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Face</th>\n",
       "      <td>0.940613</td>\n",
       "      <td>0.484699</td>\n",
       "      <td>0.940613</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.470307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Security: personal</th>\n",
       "      <td>0.647510</td>\n",
       "      <td>0.447644</td>\n",
       "      <td>0.647510</td>\n",
       "      <td>0.521236</td>\n",
       "      <td>0.639845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Security: societal</th>\n",
       "      <td>0.691571</td>\n",
       "      <td>0.408834</td>\n",
       "      <td>0.691571</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.345785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tradition</th>\n",
       "      <td>0.909962</td>\n",
       "      <td>0.476429</td>\n",
       "      <td>0.909962</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.454981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Conformity: rules</th>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.388889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Conformity: interpersonal</th>\n",
       "      <td>0.959770</td>\n",
       "      <td>0.489736</td>\n",
       "      <td>0.959770</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.479885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Humility</th>\n",
       "      <td>0.925287</td>\n",
       "      <td>0.480597</td>\n",
       "      <td>0.925287</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.462644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Benevolence: caring</th>\n",
       "      <td>0.750958</td>\n",
       "      <td>0.436304</td>\n",
       "      <td>0.750958</td>\n",
       "      <td>0.498874</td>\n",
       "      <td>0.478143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Benevolence: dependability</th>\n",
       "      <td>0.837165</td>\n",
       "      <td>0.455683</td>\n",
       "      <td>0.837165</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.418582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Universalism: concern</th>\n",
       "      <td>0.628352</td>\n",
       "      <td>0.425866</td>\n",
       "      <td>0.628352</td>\n",
       "      <td>0.513612</td>\n",
       "      <td>0.614596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Universalism: nature</th>\n",
       "      <td>0.917625</td>\n",
       "      <td>0.478521</td>\n",
       "      <td>0.917625</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.458812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Universalism: tolerance</th>\n",
       "      <td>0.863985</td>\n",
       "      <td>0.463515</td>\n",
       "      <td>0.863985</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.431992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Universalism: objectivity</th>\n",
       "      <td>0.802682</td>\n",
       "      <td>0.445271</td>\n",
       "      <td>0.802682</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.401341</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            F1-Micro  F1-Macro  Accuracy    Recall  Precision\n",
       "Self-direction: thought     0.795019  0.442903  0.795019  0.500000   0.397510\n",
       "Self-direction: action      0.752874  0.436987  0.752874  0.503846   0.876200\n",
       "Stimulation                 0.963602  0.490732  0.963602  0.500000   0.481801\n",
       "Hedonism                    0.978927  0.494676  0.978927  0.500000   0.489464\n",
       "Achievement                 0.731801  0.422566  0.731801  0.500000   0.365900\n",
       "Power: dominance            0.909962  0.476429  0.909962  0.500000   0.454981\n",
       "Power: resources            0.911877  0.476954  0.911877  0.500000   0.455939\n",
       "Face                        0.940613  0.484699  0.940613  0.500000   0.470307\n",
       "Security: personal          0.647510  0.447644  0.647510  0.521236   0.639845\n",
       "Security: societal          0.691571  0.408834  0.691571  0.500000   0.345785\n",
       "Tradition                   0.909962  0.476429  0.909962  0.500000   0.454981\n",
       "Conformity: rules           0.777778  0.437500  0.777778  0.500000   0.388889\n",
       "Conformity: interpersonal   0.959770  0.489736  0.959770  0.500000   0.479885\n",
       "Humility                    0.925287  0.480597  0.925287  0.500000   0.462644\n",
       "Benevolence: caring         0.750958  0.436304  0.750958  0.498874   0.478143\n",
       "Benevolence: dependability  0.837165  0.455683  0.837165  0.500000   0.418582\n",
       "Universalism: concern       0.628352  0.425866  0.628352  0.513612   0.614596\n",
       "Universalism: nature        0.917625  0.478521  0.917625  0.500000   0.458812\n",
       "Universalism: tolerance     0.863985  0.463515  0.863985  0.500000   0.431992\n",
       "Universalism: objectivity   0.802682  0.445271  0.802682  0.500000   0.401341"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_run_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final results F1 micro: 0.8348659003831417\n",
      "Final results F1 macro: 0.45859234932832554\n"
     ]
    }
   ],
   "source": [
    "# calculate average f1 score for the final model\n",
    "print('Final results F1 micro:', final_run_scores['F1-Micro'].mean())\n",
    "print('Final results F1 macro:', final_run_scores['F1-Macro'].mean())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "408ff3118643ffe4a17d68811b8c1004a4d886e4bbf4052e7d08910a4f0a7aa3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
